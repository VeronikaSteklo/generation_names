{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-19T19:00:14.189636Z",
     "start_time": "2025-07-19T19:00:14.130657Z"
    }
   },
   "source": [
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import Optional, Dict\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Парсинг ЛитПричал",
   "id": "46fcebba8d0c5b5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LitPrichalParser:\n",
    "    \"\"\"Парсер сайта ЛитПричал\"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str = \"https://www.litprichal.ru\"):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\"User-Agent\": UserAgent().random}\n",
    "        self.timeout = 10\n",
    "\n",
    "    def _get_page(self, url: str) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"Загружает страницу и возвращает BeautifulSoup-объект.\"\"\"\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            response = requests.get(url, headers=self.headers, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, \"html.parser\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Ошибка при загрузке {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_genres(self) -> Dict[str, Dict[str, str]]:\n",
    "        \"\"\"Парсит список жанров с главной страницы.\"\"\"\n",
    "        url = f\"{self.base_url}/prose.php\"\n",
    "        soup = self._get_page(url)\n",
    "        if not soup:\n",
    "            return {}\n",
    "\n",
    "        genres = {}\n",
    "        genre_blocks = soup.find_all(\"div\", class_=\"col-sm-6 col-md-4\")\n",
    "\n",
    "        print(\"Парсинг жанров:\")\n",
    "        for block in tqdm(genre_blocks, desc=\"Жанры\"):\n",
    "            for genre_link in block.find_all(\"a\"):\n",
    "                name = genre_link.text.strip()\n",
    "                link = urljoin(self.base_url, genre_link.get(\"href\"))\n",
    "                genres[name] = {\"link\": link}\n",
    "\n",
    "        return genres\n",
    "\n",
    "    def _get_page_count(self, genre_url: str) -> int:\n",
    "        \"\"\"Определяет количество страниц в жанре.\"\"\"\n",
    "        soup = self._get_page(genre_url)\n",
    "        if not soup:\n",
    "            return 1\n",
    "\n",
    "        pagination = soup.find(\"ul\", class_=\"pagination\")\n",
    "        if not pagination:\n",
    "            return 1\n",
    "\n",
    "        pages = pagination.find_all(\"li\")\n",
    "        if not pages:\n",
    "            return 1\n",
    "\n",
    "        try:\n",
    "            last_page = int(pages[-1].find(\"a\").get(\"href\")\n",
    "                            .strip(\"/\").split(\"/\")[-1]\n",
    "                            .replace(\"p\", \"\"))\n",
    "            return last_page\n",
    "        except (ValueError, IndexError):\n",
    "            return 1\n",
    "\n",
    "    def get_books(self, genre_url: str) -> Dict[str, Dict[str, str]]:\n",
    "        \"\"\"Парсит книги из указанного жанра с учетом пагинации.\n",
    "\n",
    "        Args:\n",
    "            genre_url: URL страницы жанра\n",
    "        \"\"\"\n",
    "        total_pages = self._get_page_count(genre_url)\n",
    "\n",
    "        books_data = {}\n",
    "\n",
    "        print(f\"\\nПарсинг книг в жанре {genre_url} (всего страниц: {total_pages}):\")\n",
    "\n",
    "        for page in range(1, total_pages + 1):\n",
    "            page_url = f\"{genre_url}/{f'p{str(page)}'}\" if page > 1 else genre_url\n",
    "            soup = self._get_page(page_url)\n",
    "            if not soup:\n",
    "                continue\n",
    "\n",
    "            books = soup.find_all(\"div\", class_=\"col-md-6 x2\")\n",
    "\n",
    "            for book in tqdm(books, desc=f\"Страница {page}/{total_pages}\"):\n",
    "                try:\n",
    "                    title = book.find(\"a\", class_=\"bigList\").text.strip()\n",
    "                    link = self.base_url + book.find(\"a\", class_=\"bigList\").get(\"href\")\n",
    "                    author = book.find(\"a\", class_=\"forum\").text.strip()\n",
    "\n",
    "                    if author not in books_data:\n",
    "                        text = self.get_text(link)\n",
    "                        books_data[author] = {\n",
    "                            \"link\": link,\n",
    "                            \"title\": title,\n",
    "                            \"text\": text,\n",
    "                            \"genre_url\": genre_url\n",
    "                        }\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nОшибка при парсинге книги: {e}\")\n",
    "                    continue\n",
    "\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        return books_data\n",
    "\n",
    "    def get_text(self, url: str) -> str:\n",
    "        \"\"\"Возвращает текст\"\"\"\n",
    "        time.sleep(0.5)\n",
    "        soup = self._get_page(url)\n",
    "        if not soup:\n",
    "            return \"\"\n",
    "\n",
    "        text_blocks = soup.find_all(\"div\", class_=\"col-md-12 x2\")\n",
    "        return self.clean_text(str(text_blocks[1]))\n",
    "\n",
    "    def clean_text(self, html: str) -> str:\n",
    "        \"\"\"Очищает HTML от ненужных элементов и возвращает чистый текст\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        for element in soup(['iframe', 'img', 'script', 'style',\n",
    "                             'div.video-blk', 'div.video-block',\n",
    "                             'div.ads', 'div.advertisement']):\n",
    "            element.decompose()\n",
    "\n",
    "        for div in soup.find_all('div', class_=lambda x: x and 'hidden' in x):\n",
    "            div.decompose()\n",
    "        clean_text = soup.get_text(separator='\\n', strip=True)\n",
    "        lines = []\n",
    "        for line in clean_text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                lines.append(line)\n",
    "\n",
    "        final_text = '\\n'.join(lines)\n",
    "\n",
    "        return final_text\n",
    "\n",
    "    def save_to_json(self, data: Dict, filename: str = \"data/парсинг/data_litprichal.json\") -> None:\n",
    "        \"\"\"Сохраняет данные в JSON-файл.\"\"\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f'\\nДанные сохранены в {filename}')\n",
    "\n",
    "    def parse_all_in_genre(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Парсит все жанры и книги в них.\"\"\"\n",
    "        genres = self.get_genres()\n",
    "        result = {}\n",
    "        print(\"\\nПарсинг книг по всем жанрам:\")\n",
    "        for genre_name, genre_data in tqdm(genres.items(), desc=\"Общий прогресс\"):\n",
    "            books = self.get_books(genre_data[\"link\"])\n",
    "            result[genre_name] = books\n",
    "            time.sleep(10)\n",
    "        return genres\n"
   ],
   "id": "ab47016e9cff36c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "parser = LitPrichalParser()\n",
    "books_data = parser.parse_all_in_genre()\n",
    "parser.save_to_json(books_data)"
   ],
   "id": "f4cc6b4020aa2d35"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Парсинг Проза.ру",
   "id": "2f62deb36be27c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T19:00:18.222609Z",
     "start_time": "2025-07-19T19:00:18.204752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ProzaRuParser:\n",
    "    \"\"\"Парсер для сайта proza.ru\"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str = \"https://proza.ru/texts/list.html\", delay: float = 1.5):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\"User-Agent\": UserAgent().random}\n",
    "        self.timeout = 10\n",
    "        self.delay = delay\n",
    "        self.output_file = \"../../data/temp_data/json/data_proza_ru.json\"\n",
    "\n",
    "        with open(self.output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def _get_page(self, url: str) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"Загружает страницу и возвращает BeautifulSoup-объект.\"\"\"\n",
    "        try:\n",
    "            print(f\"Загружается: {url}\")\n",
    "            time.sleep(self.delay)\n",
    "            response = requests.get(url, headers=self.headers, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, \"html.parser\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Ошибка при загрузке {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_all_forms(self) -> Dict[str, str]:\n",
    "        \"\"\"Получает названия и ссылки на разделы с малыми формами.\"\"\"\n",
    "        soup = self._get_page(self.base_url)\n",
    "        if not soup:\n",
    "            return {}\n",
    "\n",
    "        works_block = soup.find('ul', attrs={'type': 'square', 'style': 'color:#404040'})\n",
    "        all_forms = works_block.find_all('ul', attrs={'type': 'square'})\n",
    "        data_all_forms = {}\n",
    "        for form in all_forms:\n",
    "            category = form.find_all('a')\n",
    "            for link in category:\n",
    "                title = link.text.strip()\n",
    "                full_link = \"https://www.proza.ru\" + link['href']\n",
    "                data_all_forms[title] = full_link\n",
    "\n",
    "        return data_all_forms\n",
    "\n",
    "    def get_text(self, url: str) -> str:\n",
    "        soup = self._get_page(url)\n",
    "        if not soup:\n",
    "            return \"\"\n",
    "\n",
    "        text = soup.find('div', attrs={'class': 'text'})\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        return self.clean_text(str(text))\n",
    "\n",
    "    def clean_text(self, html: str) -> str:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for element in soup(['iframe', 'img', 'script', 'style',\n",
    "                             'div.video-blk', 'div.video-block',\n",
    "                             'div.ads', 'div.advertisement']):\n",
    "            element.decompose()\n",
    "\n",
    "        for div in soup.find_all('div', class_=lambda x: x and 'hidden' in x):\n",
    "            div.decompose()\n",
    "\n",
    "        clean_text = soup.get_text(separator='\\n', strip=True)\n",
    "        lines = [line.strip() for line in clean_text.split('\\n') if line.strip()]\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def get_works(self, url: str, category_title: str) -> Dict[str, Dict[str, str]]:\n",
    "        \"\"\"\n",
    "\n",
    "        :param url:\n",
    "        :param category_title:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        soup = self._get_page(url)\n",
    "        if not soup:\n",
    "            return {}\n",
    "\n",
    "        works_block = soup.find_all('ul', attrs={'type': 'square', 'style': 'color:#404040'})\n",
    "        data_small_works = {}\n",
    "\n",
    "        for works_list in works_block:\n",
    "            for work in works_list.find_all('li'):\n",
    "                work_data = work.find('a')\n",
    "                if not work_data:\n",
    "                    continue\n",
    "                try:\n",
    "                    author = work.find('a', attrs={'class': 'poemlink'}).text.strip()\n",
    "                    title = work_data.text.strip()\n",
    "                    link = \"https://www.proza.ru\" + work_data['href']\n",
    "                    text = self.get_text(link)\n",
    "\n",
    "                    data_small_works[author] = {\n",
    "                        'link': link,\n",
    "                        'title': title,\n",
    "                        'text': text\n",
    "                    }\n",
    "                    self._update_output_file(category_title, title, data_small_works[title])\n",
    "                    time.sleep(self.delay)\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при обработке произведения: {e}\")\n",
    "                    continue\n",
    "\n",
    "        return data_small_works\n",
    "\n",
    "    def parse_by_dates(self, start_date: str, end_date: str, category_title: str, topic: str) -> Dict[str, dict]:\n",
    "        \"\"\"\n",
    "        Парсит материалы за указанный период в обратном порядке\n",
    "        :param category_title:\n",
    "        :param start_date: Дата начала в формате 'YYYY-MM-DD'\n",
    "        :param end_date: Дата окончания в формате 'YYYY-MM-DD'\n",
    "        :param topic: ID темы\n",
    "        :return: Словарь с данными\n",
    "        \"\"\"\n",
    "        current_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "        result = {}\n",
    "\n",
    "        while current_date >= end_date:\n",
    "            date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "            print(f\"\\nОбработка даты: {date_str}\")\n",
    "\n",
    "            day = current_date.strftime(\"%d\")\n",
    "            month = current_date.strftime(\"%m\")\n",
    "            year = current_date.strftime(\"%Y\")\n",
    "\n",
    "            url = f\"{self.base_url}?day={day}&month={month}&year={year}&topic={topic}\"\n",
    "            data_day = self.get_works(url, category_title)\n",
    "            result.update(data_day)\n",
    "\n",
    "            current_date -= timedelta(days=1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _update_output_file(self, category: str, title: str, work_data: dict):\n",
    "        \"\"\"Обновляет JSON-файл, добавляя новое произведение\"\"\"\n",
    "        try:\n",
    "            with open(self.output_file, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "\n",
    "            if category not in existing_data:\n",
    "                existing_data[category] = {}\n",
    "            existing_data[category][title] = work_data\n",
    "\n",
    "            with open(self.output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка при обновлении файла: {e}\")\n",
    "\n",
    "    def get_all_work(self) -> Dict[str, Dict[str, Dict[str, str]]]:\n",
    "        \"\"\"Получает все малые формы и произведения внутри них.\"\"\"\n",
    "        all_data = {}\n",
    "        all_forms = self.get_all_forms()\n",
    "\n",
    "        for all_form_title, all_form_link in all_forms.items():\n",
    "            print(f\"\\nОбработка категории: {all_form_title}\")\n",
    "            works = self.get_works(all_form_link, all_form_title)\n",
    "            topic = re.search(r'topic=(\\d+)', all_form_link).group(1)\n",
    "            works_by_data = self.parse_by_dates(\"2025-07-18\", \"2025-07-11\", all_form_title, topic)\n",
    "            merged_works = {**works, **works_by_data}\n",
    "            all_data[all_form_title] = merged_works\n",
    "            time.sleep(self.delay)\n",
    "\n",
    "        return all_data\n",
    "\n",
    "    def save_to_json(self, data: dict, filename: str = \"data/data_proza_ru.json\"):\n",
    "        \"\"\"Сохраняет данные в JSON-файл.\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"\\n✅ Данные сохранены в {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка при сохранении JSON: {e}\")\n"
   ],
   "id": "6ad908eeffdbeadf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "parser = ProzaRuParser()\n",
    "parser.get_all_work()"
   ],
   "id": "452056176a3a4c33",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
