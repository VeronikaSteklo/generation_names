\documentclass[coursework]{SCWorks}
\usepackage{preamble}
\setminted[py]{fontsize=\small, breaklines=true, style=bw, linenos}
\newcommand{\eqdef}{\stackrel {\rm def}{=}}
\newcommand{\specialcell}[2][c]{%
    \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\renewcommand\theFancyVerbLine{\small\arabic{FancyVerbLine}}

\newtheorem{lem}{Лемма}
\linespread{1.5}
\setminted{
    fontsize=\small,          % Размер шрифта
    baselinestretch=1.0,      % Единичный интервал
    breaklines=true,          % Перенос строк
    framesep=2mm,             % Отступ вокруг кода
    linenos=false,            % Нумерация строк (опционально)
    xleftmargin=2em,          % Отступ слева
    breakanywhere=true,
}

\begin{document}
    \studentfemale


    \chair{информатики и программирования}

    \title{Генерация заголовка по краткому содержанию текста}

    \course{3}

    \group{341}
    \napravlenie{02.03.01 "--- Математическое обеспечение и администрирование информационных систем}
    \author{Загудалиной Вероники Павловны}

    \chtitle{доцент, к.\,ф.-м.\,н.}
    \chname{М.\,В.\,Огнева}

    \satitle{ст. пр.}
    \saname{А.\,А.\,Казачкова}




    \date{2026}

    \newpage

    \maketitle
    \secNumbering
    \tableofcontents


    \intro

    Современную эпоху характеризует стремительное развитие вычислительных технологий, глобальных сетей и цифровых коммуникаций,
    что ведёт к экспоненциальному росту числа пользователей Интернета и объёмов создаваемых данных.
    Согласно прогнозам, к 2025 году мировой объём цифровой информации достигнет 463 миллиардов гигабайт \cite{techjury2024}.
    Столь масштабный рост требует новых подходов к обработке, анализу и визуализации данных, которые сегодня обеспечиваются,
    в первую очередь, технологиями искусственного интеллекта (ИИ), машинного обучения (ML) и обработки естественного языка (NLP).
    Эти инструменты автоматизируют работу с текстовой информацией, делая взаимодействие человека с цифровой средой более эффективным \cite{shamigov_rugpt}.

    Особенно остро эта необходимость проявляется в сфере текстового контента — новостей, научных статей, художественных произведений
    и пользовательских публикаций, объёмы которых ежедневно увеличиваются.
    Для облегчения восприятия и повышения доступности такой информации требуются эффективные инструменты её обработки.
    Одним из ключевых среди них является автоматическая генерация заголовков, то есть создание краткого, выразительного текста,
    отражающего основную идею документа.

    В новых условиях заголовки эволюционировали: из простого средства привлечения внимания они превратились в самостоятельный
    инструмент влияния на восприятие контента.
    Теперь они выполняют такие функции, как усиление вовлечённости, повышение кликабельности, поисковая оптимизация и
    адаптация под алгоритмы социальных сетей.
    Эта трансформация тесно связана с феноменом «заголовочного чтения», когда пользователи зачастую ограничиваются лишь заголовком,
    не обращаясь к полному тексту \cite{boczkowski2018news}.

    Целью курсовой работы является проектирование и разработка системы автоматической генерации заголовков для текстовых документов.

    Задачи:
    \begin{enumerate}
        \item Изучить современные методы автоматической генерации заголовков и обзоры аналогичных исследований.
        \item Рассмотреть библиотеки и инструменты для обработки естественного языка (NLP), включая парсинг текстов и подготовку данных.
        \item Сбор и обработка текстовых данных:
        \begin{enumerate}
            \item Парсинг сайтов с литературными произведениями и публикациями.
            \item Формирование датасета с текстами и соответствующими заголовками.
        \end{enumerate}
        \item Разработка и обучение моделей генерации заголовков:
        \begin{enumerate}
            \item Реализация базовых моделей.
            \item Реализация и обучение Seq2Seq-модели.
        \end{enumerate}
        \item Оценка качества работы моделей.
    \end{enumerate}


    \section{Теоретические основы моделей генерации}

    \subsection{Сбор данных}

    Парсинг веб-страниц – это процесс извлечения данных с веб-страниц, обычно с использованием специальных программ или скриптов.

    Веб-страницы обычно написаны на языке HTML, который определяет структуру и содержание страницы.
    При парсинге веб-страниц данные извлекаются из HTML-кода, обрабатываются и преобразуются в удобный для использования формат,
    такой как текст, таблицы, JSON или XML.

    Парсинг веб-страниц может включать в себя различные этапы, такие как загрузка HTML-кода страницы, поиск нужной информации среди различных тегов и атрибутов, обработка данных и сохранение их в нужном формате.
    Для парсинга веб-страниц часто используются специализированные библиотеки и инструменты, такие как BeautifulSoup, lxml, Scrapy, Selenium и другие.

    Извлеченные данные могут использоваться для различных целей, таких как анализ, отслеживание изменений, автоматизация задач,
    создание персонализированных приложений или уведомлений.

    Однако при использовании парсинга веб-страниц важно учитывать правила использования сайта и законы о защите данных, чтобы не нарушать авторские права или правила конфиденциальности \cite{bokovikov2024weather}.

    \subsection{Предобработка и анализ текста}
    Обработка естественного языка начинается с работы с сырыми данными, которые зачастую являются неструктурированными.

    Эти данные часто содержат ошибки, неоднозначности или избыточность, что делает их сложными для анализа.
    Поэтому важным этапом является предварительная обработка текста, которая включает удаление лишних символов,
    преобразование регистра, устранение стоп-слов и другие шаги для приведения данных к стандартному виду.

    Нормализация текста означает его преобразование в более удобную, стандартную форму.
    Например, большая часть того, что мы собираемся делать с языком,
    основывается на выделении или токенизации слов из текста - задача токенизации\cite{JurafskyMartin}.

    Токенизация – это процесс разбиения фразы, предложения, абзаца или всего текстового документа на более мелкие единицы,
    например, отдельные слова или термины.
    Каждое из этих меньших подразделений называется токенами.

    Перед обработкой естественного языка нужно определить слова, которые составляют строку символов.
    В связи с этим токенизация является основным шагом для работы с NLP.
    Важность токенизации обусловлена тем, что значение текста можно легко интерпретировать,
    анализируя слова, присутствующие в тексте.

    Токенизированную форму можно использовать для подсчета базовых статистик, например,
    количества слов в тексте или частоты слова, как необходимый шаг перед более сложными шагами обработки текста\cite{Vashkevich}.

    Другой частью нормализации текста является лемматизация - задача определения того,
    что два слова имеют один и тот же корень, несмотря на их поверхностные различия.
    Например, слова пел, спетый и петь являются формами глагола петь.
    Слово петь является общей леммой этих слов, и лемматизатор переводит их все в «петь».
    Лемматизация необходима для обработки морфологически сложных языков, например, для стемминга русского языка.
    Под стеммингом понимается более простая версия лемматизации, в которой мы в основном просто удаляем суффиксы с конца слова.

    В текстах часто встречаются слова, не влияющие на смысл и лишь создающие помехи при анализе эмоциональной окраски.
    Эти слова, известные как стоп-слова, включают в себя:

    \begin{itemize}[label=\textbullet]
        \item Имена
        \item Числовые значения
        \item Вводные конструкции
        \item Служебные части речи (предлоги, частицы, союзы)
        \item Местоимения
        \item Междометия
        \item Ссылки
        \item Пунктуационные знаки
    \end{itemize}

    Для их фильтрации применяются следующие подходы:

    Методы обработки
    \begin{enumerate}
        \item {Регулярные выражения} ~--- эффективны для удаления пунктуации, цифр и других легко идентифицируемых элементов.
        \item {Предопределённые списки} ~--- популярные стоп-слова (предлоги, союзы, междометия и т.д.) заранее сохраняются в файлы, а затем загружаются в множество Python.
    \end{enumerate}

    Алгоритм удаления
    \begin{enumerate}
        \item Текст разбивается на токены методом {tokenize}.
        \item Каждый токен проверяется на вхождение в сножество стоп-слов.
        \item Совпадающие токены исключаются из дальнейшего анализа.
    \end{enumerate}

    Этот подход минимизирует шум в данных и повышает точность обработки текста\cite{AstapovMukhamadeeva}.

    \subsection{Метрики оценки качества генерации текста}

    \subsubsection{BLEU}

    Метрика BLEU (Bilingual Evaluation Understudy) была предложена в работе Папинени.
    В качестве метода автоматической оценки качества.
    Основная идея метрики заключается в том, чтобы измерять степень совпадения n-грамм (последовательностей из n слов) между
    кандидатом и одним или несколькими эталонами, выполненными человеком \cite{papineni2002bleu}.

    Ключевые компоненты алгоритма BLEU включают:

    \begin{enumerate}
        \item Модифицированная n-граммная точность: для каждого размера n-граммы (обычно от 1 до 4) вычисляется точность.
        В отличие от обычной точности, модифицированный вариант предотвращает искусственное завышение оценки за счёт многократного
        учёта одинаковых слов в кандидате.
        Количество совпадений для каждой n-граммы ограничивается максимальным количеством её появлений в любом одном эталонном переводе.
        \item Штраф за краткость: чтобы наказать слишком короткие кандидаты, которые могут иметь высокую n-граммную точность,
        но не передавать полный смысл, вводится штраф за краткость (Brevity Penalty, BP).
        \item Агрегация: итоговая оценка BLEU представляет собой взвешенное геометрическое среднее модифицированных точностей для
        разных n, умноженное на штраф за краткость.
    \end{enumerate}

    Итоговая формула для корпуса текстов имеет вид:

    \[
        BLEU = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right) \eqno(1)
    \]

    где:
    \begin{itemize}
        \item $p_n$ — модифицированная точность для n-грамм,
        \item $w_n$ — вес, обычно $w_n = 1/N$ для равномерного взвешивания,
        \item $N$ — максимальная длина n-граммы (по умолчанию 4),
        \item $BP$ — штраф за краткость.
    \end{itemize}

    \subsubsection{Perplexity}

    Для оценки качества языковых моделей часто используется показатель {Perplexity (PPL)} — функция вероятности,
    отражающая способность модели предсказывать слова в тестовом наборе данных.
    Чем лучше модель предсказывает последовательность слов, тем выше вероятность, которую она присваивает каждому слову,
    и тем ниже значение perplexity.

    Идеальная языковая модель при этом должна для каждого слова в корпусе назначать вероятность 1 (для правильного слова) и
    0 — для всех остальных.
    Однако на практике мы используем не абсолютные вероятности, а их нормированную форму, поскольку общая вероятность тестового набора
    убывает с увеличением его длины.

    Поэтому {Perplexity} служит нормированной метрикой, которая позволяет сравнивать модели, обученные на текстах различной длины.
    Она определяется как {обратная вероятность тестового набора, нормированная по количеству слов (или токенов)}.

    Для тестового корпуса $W = w_1, w_2, \dots , w_N$ perplexity вычисляется по формуле:
    \[
        \text{Perplexity}(W) = P(w_1, w_2, \ldots, w_N)^{-\frac{1}{N}} = \sqrt[N]{\frac{1}{P(w_1, w_2, \ldots, w_N)}} \eqno(2)
    \]

    Используя правило цепочки вероятностей, можно разложить вероятность последовательности W на условные вероятности отдельных слов:
    \[
        \text{Perplexity}(W) = \sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i \mid w_1, \ldots, w_{i-1})}} \eqno(3)
    \]

    Из формулы (3) следует, что чем выше вероятность последовательности слов, предсказанная моделью, тем ниже perplexity.
    Следовательно, минимизация perplexity эквивалентна максимизации вероятности тестового корпуса, что делает метрику удобным
    показателем качества языковой модели.

    Использование обратной вероятности связано с исходным определением perplexity, происходящим из коэффициента перекрёстной энтропии
    в теории информации.
    То есть perplexity имеет обратную зависимость от вероятности предсказанной последовательности \cite{jm3}.

    \subsubsection{ROUGE}


    Развитием метрик семейства BLEU, ориентированных преимущественно на показатель точности, стало появление группы метрик {ROUGE},
    которые, помимо точного совпадения элементов текста, уделяют особое внимание измерению {полноты} ({recall}).
    Показатель полноты отражает долю n-грамм, совпадающих между оригинальным и сгенерированным текстом,
    относительно общего числа n-грамм в исходном тексте.


    Формально метрика {ROUGE-N} представляет собой {recall} на уровне n-грамм между сгенерированным (candidate) резюме и
    набором эталонных (reference) резюме.
    Значение ROUGE-N вычисляется следующим образом:
    \[
        \text{ROUGE-N} =
        \frac{
            \sum_{S \in \{\text{Reference Summaries}\}}
            \sum_{\text{gram}_n \in S}
            \text{Count}_{match}(\text{gram}_n)
        }{
            \sum_{S \in \{\text{Reference Summaries}\}}
            \sum_{\text{gram}_n \in S}
            \text{Count}(\text{gram}_n)
        } \eqno(4)
    \]
    где:

    \begin{itemize}
        \item n — длина n-граммы;
        \item $\text{gram}_n$ — конкретная n-грамма длины n;
        \item $\text{Count}_{match}(\text{gram}_n)$ — максимальное количество вхождений данной n-граммы, встречающихся одновременно в сгенерированном тексте и в эталонных резюме;
        \item $\text{Count}(\text{gram}_n)$ — общее количество появлений этой n-граммы в эталонных текстах \cite{Lin2004ROUGE}.
    \end{itemize}


    Помимо классической метрики ROUGE-N, существует целый ряд модификаций, позволяющих оценивать различные аспекты текстового сходства.
    Наиболее распространённые варианты включают:
    \begin{itemize}
        \item {ROUGE-L}, основанный на длине наибольшей общей подпоследовательности слов (LCS). Данный вариант учитывает порядок слов и хорошо подходит для оценки структурного сходства предложений.
        \item {ROUGE-S}, использующий совпадение так называемых «скип-биграмм» — пар слов, которые могут быть разделены произвольным количеством других слов.
        Такой подход позволяет обнаруживать частично совпадающие структуры, даже если порядок слов в тексте был изменён \cite{Bidzhieva2024}.
    \end{itemize}

    \subsubsection{METEOR}


    Ещё одним инструментом для оценки качества автоматически создаваемого текста является метрика {METEOR}
    (Metric for Evaluation of Translation with Explicit ORdering).
    По своей сути она близка к ROUGE, однако отличается тем, что применяет заранее заданные правила выравнивания текста и
    учитывает совпадения не только на уровне точных слов, но и их синонимов.
    Такой подход позволяет более корректно оценивать случаи, когда система использует разные слова с одинаковым значением,
    а также принимать во внимание порядок слов в предложении, что делает оценку более точной.

    В отличие от BLEU, метрика METEOR опирается на несколько типов соответствия — точное совпадение слов, совпадение по корням и
    совпадение по синонимам, что делает её более чувствительной к качеству генерации на уровне фраз и предложений.
    METEOR была создана как ответ на ограничения BLEU и стремится обеспечить более высокую согласованность автоматической оценки
    с человеческими суждениями.

    По результатам экспериментов, использование METEOR на уровне словосочетаний показывает корреляцию с оценками экспертов
            {0,964}, тогда как BLEU на тех же данных достигает лишь {0,817}.
    На уровне отдельных предложений максимальная корреляция составляет {0,403} \cite{Bidzhieva2024, banarjee2005}.

    \subsection{Эволюция подходов генерации}

    \subsubsection{Шаблонные методы генерации заголовков}
    Одним из первых подходов к автоматической генерации текста и заголовков были шаблонные (template-based) системы, широко применявшиеся в 1960–1990-х годах. Принцип их работы заключался в том, что задание структуры текста и заголовка осуществлялось человеком заранее, в виде набора правил или шаблонов. В дальнейшем система лишь подставляла в эти шаблоны фактические данные.

    Основные особенности подхода:
    \begin{enumerate}
        \item Жёсткая структура: заголовок строился по заранее заданной грамматической схеме.
        \item Подстановка данных: в качестве переменных использовались ключевые сущности текста — названия команд, счёт, место, время, имена участников.
        \item Упрощение текста: для получения краткого заголовка из длинного текста часто применялись эвристики:
        \begin{itemize}
            \item удаление предлогов, местоимений и вводных слов,
            \item сохранение существительных и глаголов,
            \item сокращение до ключевых событий.
        \end{itemize}
    \end{enumerate}

    Примеры ранних систем:
    \begin{enumerate}
        \item ELIZA (1966) — имитация психотерапевта, основанная на шаблонах и правилах подстановки.
        Хотя она не генерировала заголовки, её принцип работы показывает ограниченность и прямолинейность rule-based подхода\cite{weizenbaum1966}.
        \item PARRY (1972) — симулятор пациента с паранойей, также использовавший заранее прописанные сценарии\cite{colby1972}.
        \item SHRDLU (1972) — система Терри Винограда, демонстрировавшая взаимодействие на естественном языке в ограниченном «мире блоков»; применялась грамматика и правила синтаксиса\cite{winograd1972}.
    \end{enumerate}

    Шаблонные методы активно использовались в первых новостных агрегаторах и спортивных отчётах.
    Такие заголовки генерировались автоматически на основе статистики матчей и заранее определённого набора шаблонов.

    \subsubsection{Языковые модели на основе n-грамм}

    Классические статистические языковые модели, основанные на \emph{n}-граммах, опираются на подсчёт
    частот последовательностей из $n$ токенов в корпусе.
    N-граммные языковые модели используют предположение Маркова, которое утверждает, что в контексте языкового
    моделирования вероятность следующего слова в последовательности зависит только от предыдущего(их) слова(слов)
    В простейшей форме вероятность появления токена $w_i$ при заданном контексте $w_{i-(n-1):i-1}$ оценивается как

    \begin{equation}
        P_n(w_i \mid w_{i-(n-1):i-1}) = \frac{\mathrm{cnt}(w_{i-(n-1):i-1} w_i \mid \mathcal{D})}{\mathrm{cnt}(w_{i-(n-1):i-1} \mid \mathcal{D})},\label{eq:equation}
    \end{equation}

    где $\mathrm{cnt}(\mathbf{w} \mid \mathcal{D})$ — количество появлений $n$-граммы $\mathbf{w}$ в обучающем корпусе
    $\mathcal{D}$, а $n$ — заранее заданный гиперпараметр.
    Для $n=1$ контекст $w_{i-(n-1):i-1}$ определяется как пустая строка $\varepsilon$, количество которой равно $|\mathcal{D}|$.

    Однако на практике такая наивная модель сталкивается с проблемой разреженности данных:
    числитель в формуле может быть равен нулю, что приводит к бесконечной перплексии.
    Одним из способов решения этой проблемы является стратегия backoff \cite{jurafsky2000speech}:
    если числитель равен нулю, уменьшают $n$ на единицу и повторяют процесс до тех пор, пока числитель не станет положительным.
    Следует отметить, что backoff не формирует корректное распределение $P_n(* \mid w_{i-(n-1):i-1})$,
    поскольку эффективное $n$ зависит от конкретного $w_i$,
    поэтому требуется дополнительное снижение вероятностей для нормализации распределения,
    например, метод Katz \cite{katz1987estimation}.

    Исторически n-граммовые модели реализовывались через построение таблицы подсчёта $n$-грамм по обучающему корпусу.
    Каждая уникальная $n$-грамма сохраняется вместе с числом её появлений.
    Размер таких таблиц растёт почти экспоненциально с увеличением $n$.
    Например, оценка показывает, что таблица для 5-грамм на корпусе из 1,4 триллиона токенов займёт примерно 28 ТиБ дискового пространства.
    Поэтому традиционные $n$-граммовые модели ограничены малыми значениями $n$, чаще всего $n=5$ \cite{franz2006n, aiden2011digital}.
    Малое значение $n$ приводит к потере более богатого контекста, что снижает предсказательную способность таких моделей \cite{liu2024infini}.

    \subsubsection{Feedforward Neural Network Language Model (FNNLM)}


    Одним из первых и наиболее влиятельных подходов к построению нейронных языковых моделей является {нейронная вероятностная языковая
    модель} (Neural Probabilistic Language Model), предложенная Yoshua Bengio и соавторами в 2003 году.
    Эта модель является прямой (feedforward) нейронной сетью, обучаемой предсказывать вероятность следующего слова по фиксированному
    контексту из n-1 предыдущих слов.

    Основная проблема традиционных моделей типа n-грамм заключается в {«проклятии размерности»}:
    число возможных последовательностей слов растёт экспоненциально, поэтому большинство контекстов в тестовых данных отсутствуют
    в обучающей выборке.
    В результате требуется агрессивное сглаживание или сокращение контекста, что ухудшает качество генерации.
    Bengio указывает, что n-граммы фактически «склеивают» короткие фрагменты текста и не умеют учитывать сходство слов или более
    дальние зависимости между ними.

    Для решения этих ограничений Bengio и соавторы предложили два ключевых механизма:
    \begin{enumerate}
        \item Обучение распределённых представлений слов:
        Каждому слову сопоставляется обучаемый вектор признаков — embedding.
        Тем самым слова с похожими синтаксическими и семантическими ролями оказываются близко друг к другу в пространстве признаков.
        \item {Моделирование вероятности следующего слова с помощью feedforward-сети}
        Вероятность следующего слова вычисляется как гладкая функция от векторных представлений слов контекста.
        Поскольку функция непрерывна, небольшое изменение входа ведёт к небольшому изменению вероятности, что обеспечивает
        генерализацию на новые сочетания слов.
    \end{enumerate}

    Feedforward-LM состоит из двух основных частей:
    \begin{enumerate}
        \item {Слой представлений слов}

        Матрица C размера $|V| \times m$, где каждая строка — embedding слова.

        Для входного окна ($w_{t-n+1}, \dots, w_{t-1}$) векторы конкатенируются: $x = (C(w_{t-1}), \ldots, C(w_{t-n+1}))$.
        \item {Нейронная сеть предсказания}

        Сеть с одним скрытым слоем и функцией активации tanh:
        $y = b + Wx + U \tanh(d + Hx)$,
        где $y_i$ — логиты вероятности для слова i.
    \end{enumerate}

    Выход нормализуется через softmax:

    \[
        P(w_t = i | w_{t-1}, \dots ) = \frac{e^{y_i}}{\sum_j e^{y_j}}.
    \]

    Таким образом, модель одновременно обучает:
    \begin{itemize}
        \item параметры сети,
        \item распределённые векторные представления слов.
    \end{itemize}

    Работа показывает, что использование FFNN-LM позволяет:
    \begin{enumerate}
        \item эффективно бороться с проклятием размерности,
        \item учитывать дальние зависимости (до 5–6 слов в окне),
        \item переносить знания за счёт сходства слововых embedding’ов
        (пример: предложение с «dog» имеет высокую вероятность, даже если обучалась модель на варианте с «cat») .
    \end{enumerate}

    На реальных корпусах (Brown, AP News) FFNN-модели значительно снизили перплексию по сравнению с n-граммами — до 20–30\%,
    что сделало подход архитектурно революционным \cite{Bengio2003NeuralProbLM}.

    \newpage


    \section{Работа с данными}

    \subsection{Парсинг данных}
    Веб-скрапинг (web-scraping) – это автоматизированное извлечение информации с веб-страниц.
    Например, сбор контактов из онлайн-каталога можно отнести к скрапингу.
    Поскольку ручной сбор больших массивов данных неэффективен, для этой задачи используют специальные программы – веб-скраперы.

    С правовой точки зрения веб-скрапинг, как правило, не считается нарушением закона, так как он работает с общедоступными данными.

    С ростом цифровизации значение веб-скрапинга увеличилось.
    Согласно исследованию компании, специализирующейся на информационной безопасности,
    более 50\% (52\%) интернет-трафика (без учета аудио- и видеопотоков) генерируется ботами – автоматизированными системами\cite{moskalenko}.

    Для построения и обучения моделей машинного обучения требуется корпус данных, соответствующий предметной области исследования.
    Так как готовые наборы данных не всегда удовлетворяют требованиям по объёму, актуальности и тематике, было принято решение осуществить автоматизированный сбор текстовой информации из открытых источников.
    Основная цель парсинга — формирование репрезентативного набора текстов, который может быть использован для последующей предобработки и обучения моделей.

    В качестве источников данных были выбраны:
    \begin{enumerate}
        \item Сайты с произведениями:
        \begin{itemize}
            \item Проза.ру
            \item ЛитПричал
        \end{itemize}
        \item Литрес — сайт по продаже книг, в котором для каждой книги писалась аннотация.
        \item Брифли — сайт с краткими пересказами произведений.
    \end{enumerate}

    Критериями выбора послужили: доступность данных, достаточный объём информации, а также структурированность представления материалов.
    Для разработки парсеров были использованы следующие инструменты и библиотеки Python:
    \begin{itemize}
        \item requests — для получения HTML-кода страниц;
        \item BeautifulSoup — для анализа и извлечения информации из HTML;
        \item fake\_useragent — для имитации различных User-Agent при отправке запросов и снижения риска блокировки;
        \item json — для сохранения собранных данных в структурированном формате;
        \item time, datetime — для управления задержками между запросами и работы с датами публикаций;
        \item tqdm — для визуализации прогресса выполнения парсинга.
    \end{itemize}
    Для каждого сайта был написан отдельный парсер.
    Перед написанием была изучена структура HTML документа каждого из сайтов для корректного сбора нужной информации.
    Рассмотрим подробно каждый.

    \subsubsection{Парсинг Проза.ру}
    Основной целью парсинга являлось получение текстов вместе с метаданными для
    последующей предобработки и обучения моделей машинного обучения.

    Парсер реализован как класс ProzaRuParser со следующими основными методами:

    Метод \texttt{\_\_init\_\_}: при создании объекта класса задаются базовый URL, заголовки запроса, время для ожидания ответа от сайта,
    задержка для запросов, чтобы не перегружать сайт, и файл для сохранения данных:
    \begin{minted}{text}
def __init__(self, base_url: str = "https://proza.ru/texts/list.html", delay: float = 1.5):
    self.base_url = base_url
    self.headers = {"User-Agent": UserAgent().random}
    self.timeout = 10
    self.delay = delay
    self.output_file = "../data/temp_data/json/data_proza_ru.json"

    with open(self.output_file, 'w', encoding='utf-8') as f:
        json.dump({}, f, ensure_ascii=False, indent=4)
    \end{minted}

    Метод \texttt{\_get\_page}: загружает страницу и возвращает BeautifulSoup-объект.
    \begin{minted}{text}
time.sleep(self.delay)
response = requests.get(url, headers=self.headers, timeout=self.timeout)
response.raise_for_status()
return BeautifulSoup(response.content, "html.parser")
    \end{minted}


    Метод \texttt{get\_all\_forms()}: получает все категории, представленные на сайте Proza.ru.

    Сначала он загружает HTML-код основной страницы с помощью вспомогательного метода \_get\_page().
    Далее из полученного документа извлекается блок с разделами произведений,
    после чего метод проходит по каждому подразделу и собирает ссылки на все категории.
    Для каждой категории сохраняется название и полная ссылка на соответствующую страницу.
    В результате метод возвращает словарь, где ключом является название категории, а значением — URL страницы.

    \begin{minted}{text}
def get_all_forms(self) -> Dict[str, str]:
    """Получает названия и ссылки на разделы с малыми формами."""
    soup = self._get_page(self.base_url)
    if not soup:
        return {}

    works_block = soup.find('ul', attrs={'type': 'square', 'style': 'color:#404040'})
    all_forms = works_block.find_all('ul', attrs={'type': 'square'})
    data_all_forms = {}
    for form in all_forms:
        category = form.find_all('a')
        for link in category:
            title = link.text.strip()
            full_link = "https://www.proza.ru" + link['href']
            data_all_forms[title] = full_link

    return data_all_forms
    \end{minted}

    Метод \texttt{clean\_text()} выполняет очистку HTML-контента и извлечение чистого текста из страницы.

    Сначала метод создаёт объект \texttt{BeautifulSoup} для анализа HTML-кода.
    Затем удаляются все нерелевантные элементы, такие как: \texttt{iframe}, \texttt{img}, \texttt{script}, \texttt{style}, рекламные блоки (\texttt{div.ads}, \texttt{div.advertisement}) и блоки видео (\texttt{div.video-blk}, \texttt{div.video-block}).
    Дополнительно удаляются скрытые элементы с классом, содержащим \texttt{'hidden'}.
    После удаления всех лишних элементов текст извлекается с сохранением разделения строк, а пустые строки удаляются.
    Метод возвращает очищенный текст в виде строки.

    \begin{minted}{text}
def clean_text(self, html: str) -> str:
    soup = BeautifulSoup(html, 'html.parser')
    for element in soup(['iframe', 'img', 'script', 'style',
                         'div.video-blk', 'div.video-block',
                         'div.ads', 'div.advertisement']):
        element.decompose()

    for div in soup.find_all('div', class_=lambda x: x and 'hidden' in x):
        div.decompose()

    clean_text = soup.get_text(separator='\n', strip=True)
    lines = [line.strip() for line in clean_text.split('\n') if line.strip()]
    return '\n'.join(lines)
    \end{minted}

    Метод \texttt{get\_works()} выполняет сбор всех произведений в рамках выбранной категории на сайте Proza.ru.

    Сначала метод загружает HTML-страницу категории с помощью вспомогательного метода \_get\_page().
    Затем из документа извлекаются блоки с перечнем произведений.
    Для каждого произведения метод получает:
    \begin{enumerate}
        \item имя автора (\texttt{author});
        \item заголовок произведения (\texttt{title});
        \item ссылку на полную страницу (\texttt{link});
        \item текст произведения (\texttt{text}), который загружается и очищается с помощью метода \texttt{get\_text()}.
    \end{enumerate}

    Собранные данные сохраняются в словарь, где ключом является имя автора, а значением — словарь с заголовком, ссылкой и текстом произведения.
    После обработки каждого произведения данные также добавляются в JSON-файл с помощью метода \texttt{\_update\_output\_file()}, а между запросами делается задержка для предотвращения блокировки.

    Метод возвращает словарь с произведениями категории.

    \begin{minted}{text}
def get_works(self, url: str, category_title: str) -> Dict[str, Dict[str, str]]:
    soup = self._get_page(url)
    if not soup:
        return {}

    works_block = soup.find_all('ul', attrs={'type': 'square', 'style':'color:#404040'})
    data_small_works = {}

    for works_list in works_block:
        for work in works_list.find_all('li'):
            work_data = work.find('a')
            if not work_data:
                continue
            try:
                author = work.find('a', attrs={'class':'poemlink'}).text.strip()
                title = work_data.text.strip()
                link = "https://www.proza.ru" + work_data['href']
                text = self.get_text(link)

                data_small_works[author] = {
                    'link': link,
                    'title': title,
                    'text': text
                }
                self._update_output_file(category_title, title, data_small_works[title])
                time.sleep(self.delay)
            except Exception as e:
                print(f"Ошибка при обработке произведения: {e}")
                continue

    return data_small_works
    \end{minted}

    Метод \texttt{parse\_by\_dates()} выполняет сбор произведений за указанный период по дате публикации.

    На вход метод получает:
    \begin{enumerate}
        \item \texttt{start\_date} — дата начала периода в формате \texttt{'YYYY-MM-DD'};
        \item \texttt{end\_date} — дата окончания периода в том же формате;
        \item \texttt{category\_title} — название категории произведений;
        \item \texttt{topic} — идентификатор темы на сайте.
    \end{enumerate}

    Метод преобразует даты в объекты \texttt{datetime} и организует цикл, который проходит по каждому дню периода в обратном порядке.
    Для каждой даты формируется URL с указанием дня, месяца, года и темы.
    Затем вызывается метод \texttt{get\_works()}, который собирает все произведения на этой странице.
    Собранные данные добавляются в общий словарь \texttt{result}.
    В конце работы метод возвращает словарь со всеми произведениями за указанный период, структурированный по авторам и названиям.

    \begin{minted}{text}
def parse_by_dates(self, start_date: str, end_date: str, category_title: str, topic: str) -> Dict[str, dict]:
    current_date = datetime.strptime(start_date, "%Y-%m-%d")
    end_date = datetime.strptime(end_date, "%Y-%m-%d")
    result = {}

    while current_date >= end_date:
        date_str = current_date.strftime("%Y-%m-%d")
        print(f"\nОбработка даты: {date_str}")

        day = current_date.strftime("%d")
        month = current_date.strftime("%m")
        year = current_date.strftime("%Y")

        url = f"{self.base_url}?day={day}&month={month}&year={year}&topic={topic}"
        data_day = self.get_works(url, category_title)
        result.update(data_day)

        current_date -= timedelta(days=1)

    return result
    \end{minted}

    Метод \texttt{get\_all\_work()} обеспечивает полный сбор всех форм и произведений внутри них на сайте Proza.ru.

    Сначала метод получает список всех категорий форм с помощью метода \texttt{get\_all\_forms()}.
    Далее для каждой категории выполняются следующие действия:
    \begin{enumerate}
        \item вызывается метод \texttt{get\_works()} для получения всех произведений, доступных на странице категории;
        \item извлекается идентификатор темы (\texttt{topic}) из URL категории;
        \item вызывается метод \texttt{parse\_by\_dates()} для сбора произведений за определённый период по дате публикации;
        \item объединяются результаты обоих методов в один словарь \texttt{merged\_works}, который содержит все произведения категории;
        \item обновлённый словарь добавляется в общий словарь \texttt{all\_data}, где ключом является название категории.
    \end{enumerate}

    В результате метод возвращает полный словарь, структурированный по категориям, авторам и названиям произведений.

    \begin{minted}{text}
def get_all_work(self) -> Dict[str, Dict[str, Dict[str, str]]]:
    """Получает все малые формы и произведения внутри них."""
    all_data = {}
    all_forms = self.get_all_forms()

    for all_form_title, all_form_link in all_forms.items():
        print(f"\nОбработка категории: {all_form_title}")
        works = self.get_works(all_form_link, all_form_title)
        topic = re.search(r'topic=(\d+)', all_form_link).group(1)
        works_by_data = self.parse_by_dates("2025-07-18", "2025-07-11", all_form_title, topic)
        merged_works = {**works, **works_by_data}
        all_data[all_form_title] = merged_works
        time.sleep(self.delay)

    return all_data
    \end{minted}

    Метод \texttt{\_update\_output\_file()} отвечает за обновление JSON-файла при добавлении нового произведения.

    Он выполняет следующие действия:
    \begin{enumerate}
        \item считывает существующие данные из файла JSON;
        \item проверяет, существует ли категория произведения, и при необходимости создаёт новый словарь для неё;
        \item добавляет или обновляет запись с заголовком произведения и его данными (\texttt{author, title, text, link});
        \item сохраняет обновлённый словарь обратно в JSON-файл.
    \end{enumerate}

    В случае ошибки при чтении или записи файла метод выводит сообщение об ошибке.

    \begin{minted}{text}
def _update_output_file(self, category: str, title: str, work_data: dict):
    """Обновляет JSON-файл, добавляя новое произведение"""
    try:
        with open(self.output_file, 'r', encoding='utf-8') as f:
            existing_data = json.load(f)

        if category not in existing_data:
            existing_data[category] = {}
        existing_data[category][title] = work_data

        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=4)

    except Exception as e:
        print(f"❌ Ошибка при обновлении файла: {e}")
    \end{minted}

    Метод \texttt{save\_to\_json()} сохраняет весь собранный корпус данных в отдельный JSON-файл.

    Он принимает на вход словарь с данными и имя файла для сохранения.
    В случае успешного сохранения выводится сообщение с подтверждением.
    Если возникает ошибка при записи файла, метод выводит сообщение об ошибке.

    \begin{minted}{text}
def save_to_json(self, data: dict, filename: str = "data/data_proza_ru.json"):
    """Сохраняет данные в JSON-файл."""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        print(f"\nДанные сохранены в {filename}")
    except Exception as e:
        print(f"Ошибка при сохранении JSON: {e}")
    \end{minted}

    \subsubsection{Парсинг ЛитПричал}


    Данный парсер реализует сбор текстов и метаданных с сайта «ЛитПричал» и принципиально отличается от парсинга Proza.ru
    рядом особенностей архитектуры ресурса.
    Основным объектом структуры являются жанры, а не категории.
    Это определяет общую логику обхода страниц.

    Парсер реализован в виде класса \texttt{LitPrichalParser}, где в конструкторе задаются базовый URL, заголовки и таймауты запросов
    \begin{minted}{text}
def __init__(self, base_url: str = "https://www.litprichal.ru"):
    self.base_url = base_url
    self.headers = {"User-Agent": UserAgent().random}
    self.timeout = 10
    \end{minted}

    Ключевым отличием является метод \texttt{get\_genres()}.
    Он извлекает с главной страницы списка прозы все доступные жанровые разделы.
    Жанры представлены в блоках:

    \begin{minted}{text}
<div class="col-sm-6 col-md-4">...</div>
    \end{minted}

    Каждый жанр содержит одну или несколько ссылок на подразделы.
    Метод собирает названия и формирует абсолютные ссылки через urljoin.

    \begin{minted}{text}
def get_genres(self) -> Dict[str, Dict[str, str]]:
    url = f"{self.base_url}/prose.php"
    soup = self._get_page(url)

    genre_blocks = soup.find_all("div", class_="col-sm-6 col-md-4")
    for block in genre_blocks:
        for genre_link in block.find_all("a"):
            name = genre_link.text.strip()
            link = urljoin(self.base_url, genre_link.get("href"))
            genres[name] = {"link": link}
    \end{minted}


    В отличие от Proza.ru, где категории группировались в отдельные HTML-списки, здесь структура линейно распределена по bootstrap-блокам.


    Сайт ЛитПричал использует постраничную навигацию внутри жанров.
    Для определения количества страниц применяется метод \texttt{\_get\_page\_count()}.

    \begin{minted}{text}
def _get_page_count(self, genre_url: str) -> int:
    pagination = soup.find("ul", class_="pagination")
    pages = pagination.find_all("li")
    last_page = int(pages[-1].find("a").get("href").strip("/").split("/")[-1].replace("p", ""))
    \end{minted}


    Таким образом, логика сбора на ЛитПричале строится на последовательном переборе страниц.


    Произведения собираются методом \texttt{get\_books()}.
    На каждой странице ищутся элементы:
    \begin{minted}{text}
<div class="col-md-6 x2">...</div>
    \end{minted}


    Из каждого извлекаются заголовок, ссылка и имя автора.


    Для извлечения текста используется метод \texttt{get\_text()}.
    Он выбирает второй блок div.col-md-12 x2, где находится основной текст.

    \begin{minted}{text}
text_blocks = soup.find_all("div", class_="col-md-12 x2")
return self.clean_text(str(text_blocks[1]))
    \end{minted}


    На Proza.ru текстовая часть извлекалась из более сложной структуры страницы; здесь доступ проще и локализован в одном блоке.


    Метод \texttt{clean\_text()} структурно похож на аналогичный в Proza.ru, отличается отсутствием обработки некоторых специфичных элементов.


    В отличие от Proza.ru отсутствует инкрементальное обновление JSON-файла.
    Все данные собираются в оперативной памяти и записываются по завершении.


    Финальный сбор производится методом \texttt{parse\_all\_in\_genre()}.
    Он получает все жанры, затем вызывает \texttt{get\_books()} для каждого:
    \begin{minted}{text}
for genre_name, genre_data in genres.items():
    books = self.get_books(genre_data["link"])
    result[genre_name] = books
    \end{minted}


    В целом логика данного парсинга адаптирована под архитектуру сайта ЛитПричал и нацелена на сбор произведений по жанрам
    с учётом пагинации, без анализа дат публикации и без инкрементального сохранения данных.

    \subsubsection{Парсинг Литрес}

    Парсер сайта ЛитРес предназначен для сбора метаданных о книгах и получения коротких текстовых аннотаций со страниц произведений.
    В отличие от сайтов Proza.ru и ЛитПричал, ресурс ЛитРес содержит преимущественно коммерческий контент и использует более сложную верстку,
    что усложняет извлечение информации.

    Парсер реализован в виде класса \texttt{LitresParser}.
    При инициализации задаются базовый URL жанровой страницы, заголовки HTTP-запросов, таймауты и путь для сохранения результатов в JSON-файл.

    \begin{minted}{text}
def init(self, url: str):
    self.litres_url = “https://www.litres.ru”
    self.base_url = url
    self.headers = {“User-Agent”: UserAgent().random}
    self.timeout = 10
    self.filename = “/data/temp_data/litres.json”
    \end{minted}

    Основной вспомогательный метод \texttt{\_get\_page()} отвечает за загрузку HTML-страницы.
    Он осуществляет запрос к серверу с задержкой, обрабатывает сетевые ошибки и возвращает объект \texttt{BeautifulSoup}:

    \begin{minted}{text}
def _get_page(self, url: str) -> Optional[BeautifulSoup]:
    time.sleep(1.5)
    response = requests.get(url, headers=self.headers, timeout=self.timeout)
    response.raise_for_status()
    return BeautifulSoup(response.content, “html.parser”)
    \end{minted}

    Для обхода каталога и извлечения списка книг используется метод \texttt{get\_books()}.
    Он поддерживает пагинацию: на вход передается целевое количество страниц, после чего метод формирует URL вида: \texttt{<base\_url>?page=<номер>}

    На каждой странице ищутся карточки произведений:

    \begin{minted}{text}
<div class="Art-module__3wrtfG__content Art-module__3wrtfG__content_full">...</div>
    \end{minted}

    Из каждого блока извлекаются:
    \begin{itemize}
        \item заголовок книги;
        \item ссылка на страницу произведения;
        \item текст аннотации (через метод \texttt{get\_text()}).
    \end{itemize}

    При этом предусмотрена проверка на дублирование: если книга уже присутствует в ранее сохраненных данных, повторная обработка не выполняется.

    \begin{minted}{text}
for book in books:
    info = book.find(“a”, class_=“ArtInfo-module__Y-DtKG__title”)
    title = info.text.strip()
    link = self.litres_url + info.get(“href”)
    if title in books_data:
        continue
    \end{minted}

    Извлечение текстовой информации организовано в методе \texttt{get\_text()}.
    Аннотация находится внутри блока:

    \begin{minted}{text}
<div class="BookDetailsAbout-module__p8ABVW__truncate">...</div>
    \end{minted}

    и затем уточняется вложенный элемент:

    \begin{minted}{text}
truncated = block.find(“div”, class_=“Truncate-module__FwxwPG__truncated”)
return truncated.text if truncated else None
    \end{minted}

    В отличие от Proza.ru и ЛитПричала, на ЛитРес нельзя получить полный текст произведения из-за авторских прав и ограничений доступа.
    Поэтому парсер извлекает только доступные публичные аннотации.

    Сохранение результатов производится инкрементально после каждой обработанной страницы.
    Метод \texttt{save\_to\_json()} записывает текущий словарь в файл:

    \begin{minted}{text}
def save_to_json(self, data: Dict, filename: str = None) -> None:
    with open(filename or self.filename, “w”, encoding=“utf-8”) as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    \end{minted}

    Перед обработкой выполняется загрузка уже имеющегося JSON-файла:

    \begin{minted}{text}
def _load_existing_data(self) -> Dict:
    try:
        with open(self.filename, “r”, encoding=“utf-8”) as f:
            return json.load(f)
    except:
        return {}
    \end{minted}

    Таким образом, структура данных постепенно дополняется, что позволяет продолжать сбор даже при прекращении работы или сетевых ошибках.

    В целом логика парсинга ЛитРес характеризуется следующими особенностями:

    \begin{itemize}
        \item опора на пагинацию по параметру \texttt{page};
        \item сохранение данных после каждой итерации;
        \item невозможность получения полного текста произведения;
        \item извлечение только открытых аннотаций;
        \item проверка на дубликаты по названию книги.
    \end{itemize}

    Такой подход обеспечивает устойчивость обработки и минимальные потери данных при длинных сериях запросов.

    \subsubsection{Парсинг Briefly}

    Парсер, реализованный для ресурса Briefly.ru, предназначен для извлечения кратких пересказов произведений мировой литературы.

    Функциональность организована в классе \texttt{ParsingBriefly}.
    В конструкторе задаются базовый URL, заголовки HTTP-запросов, сетевые параметры и путь сохранения результатов:

    \begin{minted}{text}
def init(self, url: str):
    self.briefly_url = “https://briefly.ru”
    self.base_url = url
    self.headers = {“User-Agent”: UserAgent().random}
    self.timeout = 5
    self.filename = “../data/temp_data/json/briefly.json”
    \end{minted}

    Получение HTML-страниц осуществляется вспомогательным методом \texttt{\_get\_page()}, который генерирует задержки между
    запросами для имитации поведения человека:

    \begin{minted}{text}
def _get_page(self, url: str) -> Optional[BeautifulSoup]:
    self.human_delay()
    response = requests.get(url, headers=self.headers, timeout=self.timeout)
    return BeautifulSoup(response.content, “html.parser”)
    \end{minted}

    Использование искусственных задержек снижает риск блокировки со стороны сервера.


    Метод \texttt{get\_all\_data()} обходит карточки культур (например, русская, французская, античная), находящиеся на корневой
    странице Briefly.ru.
    Для каждой культуры извлекаются имена авторов:

    \begin{minted}{text}
cultures_cards = soup.find_all(“a”, class_=“visited-hidden”)
for culture in cultures_cards[7:]:
    name = culture.get_text(strip=True)
    link = self.briefly_url + culture.get(“href”)
    data_culture = self.get_authors(link, name)
    \end{minted}

    Таким образом обеспечивается последовательный обход каталога.

    Парсер использует метод \texttt{get\_works()}, который распознаёт две возможные структуры страницы:
    \begin{itemize}
        \item \texttt{works\_index} — содержит список полных и кратких пересказов;
        \item \texttt{author\_works} — альтернативное оформление профиля автора.
    \end{itemize}

    Дополнительно выполняется фильтрация технических названий (например, «глава», «том», «действие»),
    чтобы не загружать фрагменты произведений:

    \begin{minted}{text}
if any(word in title.lower() for word in [“глава”, “том”, “действие”]):
    continue
    \end{minted}

    Извлечение текста реализовано в методе \texttt{get\_text()}, который учитывает различные варианты вёрстки страниц:

    \begin{itemize}
        \item краткие пересказы (\texttt{pending}) находятся в \texttt{div.microsummary\_\_content};
        \item полные пересказы (\texttt{published}) — в \texttt{p.microsummary\_\_content};
        \item если формат иной — применяется резервный поиск в \texttt{div#text}.
    \end{itemize}

    Перед сохранением нежелательные рекламные элементы удаляются:

    \begin{minted}{text}
for ad in main_div.find_all(“div”, class_=“honey”):
    ad.decompose()
    \end{minted}

    Завершающий текст формируется объединением всех параграфов.

    Отличительной особенностью является метод \texttt{human\_delay()}, который:
    \begin{itemize}
        \item создаёт случайные задержки между запросами;
        \item с небольшой вероятностью инициирует длинную паузу;
        \item делает парсер менее «роботоподобным».
    \end{itemize}

    \begin{minted}{text}
delay = random.uniform(base, base + var)
if random.random() < long_pause_prob:
    time.sleep(random.uniform(5, 15))
    \end{minted}

    Подобный механизм снижает риск антибот-ограничений.

    Для сохранения прогресса используется подход постепенной записи в JSON-файл:

    \begin{minted}{text}
self.save_to_json(all_data)
    \end{minted}

    В случае прерывания процесса данные не теряются, и загрузка продолжается с последнего сохранённого состояния.

    Разработка обладает рядом отличительных черт:

    \begin{itemize}
        \item глубокая вложенность навигации (культура → автор → произведения);
        \item обработка нескольких шаблонов вёрстки страниц;
        \item фильтрация неполных текстов (глав, томов, действий);
        \item удаление рекламных блоков;
        \item антибот-стратегия (случайные паузы);
        \item инкрементальное сохранение данных.
    \end{itemize}

    Благодаря этому достигается устойчивость работы при обработке большого количества страниц и минимизируется риск блокировок.

    \subsection{Предобработка и анализ данных}

    На каждом из этих сайтов тексты имели различный формат представления, поэтому на первом этапе требовалось привести их
    к единому виду и очистить от лишней информации.


    Были написаны вспомогательные функции для открытия и преобразования данных из формата JSON в DataFrame.

    Были реализованы две функции:
    \begin{enumerate}
        \item open\_json() — открывает JSON-файл и возвращает его содержимое в виде словаря.
        \begin{minted}{text}
def open_json(input_file: str) -> dict:
    with open(input_file) as json_file:
        data = json.load(json_file)
    return data
        \end{minted}
        \item json\_to\_csv() и json\_to\_csv\_lp() — преобразуют данные разных форматов (в зависимости от сайта-источника)
        в таблицу с двумя основными столбцами:

        \begin{itemize}
            \item title — название произведения,
            \item text — текст произведения или его описание.
        \end{itemize}
        \begin{minted}{text}
def json_to_csv_lp(json_data: dict) -> DataFrame:
    rows = []
    for category, works in json_data.items():
        for author, work_data in works.items():
            rows.append({
                "title": work_data.get("title", ""),
                "text": work_data.get("text", "")
            })

    df = pd.DataFrame(rows)
    return df

def json_to_csv(json_data: dict) -> DataFrame:
    rows = []
    for title, text in json_data.items():
        rows.append({
            "title": title,
            "text": text
        })

    df = pd.DataFrame(rows)
    return df
        \end{minted}
    \end{enumerate}

    Далее все таблицы были объединены в один общий датасет:

    \begin{minted}{text}
data = pd.concat([briefly, litprichal, proza_ru, litres], ignore_index=True)
    \end{minted}


    Проверка количества записей показала, что данные объединились без потерь.


    На этапе первичной очистки все текстовые данные были приведены к нижнему регистру и очищены от лишних пробелов:
    \begin{minted}{text}
data = data.apply(lambda col: col.str.lower().str.strip())
    \end{minted}


    Затем были удалены пропущенные значения (NaN) и пустые строки.
    Это позволило избавиться от записей, в которых отсутствовали либо текст, либо название.

    Для текстов описаний была создана функция clean\_text(), которая выполняла следующие действия:

    \begin{itemize}
        \item удаляла ссылки (http), HTML-теги и спецсимволы;
        \begin{minted}{text}
text = re.sub(r"http\S+", "", text)
        \end{minted}
        \item заменяла несколько пробелов на один;
        \begin{minted}{text}
text = re.sub(r"<[^>]+>", "", text)
        \end{minted}
        \item оставляла только буквы, цифры и основные знаки пунктуации.
        \begin{minted}{text}
text = re.sub(r"[^\w\s,.!?-]", " ", text)
        \end{minted}
    \end{itemize}

    Результат — тексты были приведены к единому читаемому виду, пригодному для анализа и подачи модели.

    Во многих названиях встречались элементы нумерации, такие как «том 2», «глава 5», «эпизод 3» и т.п.
    Для их удаления была реализована функция \texttt{clean\_title\_completely}.


    \texttt{clean\_title\_completely(title)} — функция для «полной» очистки названия произведения от любых форм нумерации (тома, главы, части,
    эпизоды и т.п.), а также от сопутствующего шумового мусора (скобки с числами, служебные символы, лишняя пунктуация и т.д.).
    Результат — компактная читабельная строка, пригодная как целевая метка для обучения модели генерации заголовков.

    \begin{minted}{text}
cleaned = remove_complex_volume_numbers(title)
cleaned = str(cleaned).lower().strip()
    \end{minted}


    Complex\_patterns используется для удаления более сложных конструкций нумерации, включающих сочетания нескольких типов меток
    (том, часть, эпизод, серия, глава, книга) и двух и более числовых значений.

    \begin{minted}{text}
complex_patterns = [
        r'\(?\s*\d+\s+(?:эпизод|серия|глава|часть)\s+\d+\s+(?:том|книга|т\.)\s*\d*\s*\.?\)?',
        r'\(?\s*\d+\s+(?:том|книга|т\.)\s+\d+\s+(?:эпизод|серия|глава|часть)\s*\d*\s*\.?\)?',
        r'\b\d+\s+\d+\s+(?:том|часть|книга|эпизод|серия)\b',
        r'\b(?:том|часть|книга|эпизод|серия)\s+\d+\s+\d+\b',
        r'\b\d+[-\.,]\s*\d+\s+(?:том|часть|книга)',
        r'\b(?:том|часть|книга)\s+\d+[-\.,]\s*\d+\b',
    ]

for pattern in complex_patterns:
    cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
    \end{minted}

    \begin{enumerate}
        \item \verb~\(?\s*\d+\s+(?:эпизод|серия|глава|часть)\s+\d+\s+(?:том|книга|т\.)~\\ \verb~\s*\d*\s*\.?\)?~
        \begin{itemize}
            \item Ищет сложные конструкции вида: {число + тип контента + число + тип издания};
            \item Например, (12 серия 3 том), 5 глава 2 книга.
        \end{itemize}

        \item \verb~\(?\s*\d+\s+(?:том|книга|т\.)\s+\d+\s+(?:эпизод|серия|глава|часть)~\\ \verb~\s*\d*\s*\.?\)?~
        \begin{itemize}
            \item Обратный порядок: {число + тип издания + число + тип контента};
            \item Например, (3 том 12 серия), 2 книга 5 глава.
        \end{itemize}

        \item \verb~\b\d+\s+\d+\s+(?:том|часть|книга|эпизод|серия)\b~
        \begin{itemize}
            \item Ищет два числа подряд + тип контента/издания;
            \item Например, 12 3 том, 5 2 книга.
        \end{itemize}

        \item \verb~\b(?:том|часть|книга|эпизод|серия)\s+\d+\s+\d+\b~
        \begin{itemize}
            \item Обратный порядок: {тип контента/издания + два числа};
            \item Например, том 12 3, книга 5 2.
        \end{itemize}

        \item \verb~\b\d+[-\.,]\s*\d+\s+(?:том|часть|книга)~
        \begin{itemize}
            \item Ищет числа с разделителями + тип издания;
            \item Например, 12-3 том, 5.2 книга.
        \end{itemize}

        \item \verb~\b(?:том|часть|книга)\s+\d+[-\.,]\s*\d+\b~
        \begin{itemize}
            \item Обратный порядок: {тип издания + числа с разделителями};
            \item Например, том 12-3, книга 5.2, часть 10,1.
        \end{itemize}
    \end{enumerate}

    Basic\_patterns используется для того, чтобы удалить распространённые сочетания слов-меток (том, часть, глава, книга,
    эпизод, серия, выпуск, т.) вместе с последующими/предшествующими цифрами или римскими цифрами.

    \begin{minted}{text}
basic_patterns = [
    r'\s*(?:том|часть|книга|т\.|vol\.?|эпизод|серия|глава|выпуск)\s*[ivxlcdm0-9]+',
    r'\s*[ivxlcdm0-9]+\s*(?:том|часть|книга|т\.|vol\.?|эпизод|серия|глава|выпуск)',
    r'\s*\d+[-\.,]?\s*(?:том|часть|книга|глава)',
    r'\s*(?:том|часть|книга|глава)[-\.,]?\s*\d+',
]
for pattern in basic_patterns:
    cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
    \end{minted}

    \begin{enumerate}
        \item \verb~\s*(?:том|часть|книга|т\.|vol\.?|эпизод|серия|глава|выпуск)\s*[ivxlcdm0-9]+~
        \begin{itemize}
            \item Ищет слово типа том (или сокращение т., vol.) + пробелы + число (арабское) или римское (ivxlcdm);
            \item \verb|\s*| до и после — чтобы удалить предшествующий пробел и не оставлять двойных пробелов.
        \end{itemize}

        \item \verb~\s*[ivxlcdm0-9]+\s*(?:том|часть|книга|т\.|vol\.?|эпизод|серия|глава|~ \\\verb~выпуск)~
        \begin{itemize}
            \item Обратная форма: число перед словом;
            \item Например, 2 том, iii том.
        \end{itemize}

        \item \verb~\s*\d+[-\.,]?\s*(?:том|часть|книга|глава)~
        \begin{itemize}
            \item Захватывает варианты с разделителями между числом и словом;
            \item Например, 10-том, 10, том, 10. том.
        \end{itemize}

        \item \verb~\s*(?:том|часть|книга|глава)[-\.,]?\s*\d+~
        \begin{itemize}
            \item Аналогичный случай, когда разделитель стоит после слова;
            \item Например, том-10, том. 10.
        \end{itemize}
    \end{enumerate}

    Для того чтобы удалить текстовые порядковые обозначения (глава вторая, часть первая и т.д.) был написан паттерн:

    \begin{minted}{text}
number_words = ["первая", "вторая", "третья", ... , "двадцатая"]
text_num_pattern = r'\b(?:глава|часть|том|эпизод|серия|книга|выпуск)\s+(?:' + "|".join(number_words) + r')\b'
cleaned = re.sub(text_num_pattern, '', cleaned, flags=re.IGNORECASE)
    \end{minted}

    Для удаления форм с русскими окончаниями (падежные и порядковые суффиксы): 2-я глава, 3й том, 4ая часть, том 5-ая и т.п., были написаны паттерны:

    \begin{minted}{text}
cleaned = re.sub(
    r'\b\d+[-–]?(?:я|й|е|ой|ая|ое|ые|ых)?\s+(?:глава|часть|том|книга|серия|эпизод|выпуск)\b',
    '', cleaned, flags=re.IGNORECASE
)
cleaned = re.sub(
    r'\b(?:глава|часть|том|книга|серия|эпизод|выпуск)\s+\d+[-–]?(?:я|й|е|ая|ое|ые|ых)?\b',
    '', cleaned, flags=re.IGNORECASE
)
    \end{minted}

    \begin{enumerate}
        \item \verb|[-–]?| учитывает разные дефисы/тире (обычный - и длинный –);
        \item \verb~(?:я|й|е|ой|ая|ое|ые|ых)?~ — возможные окончания, которые часто встречаются у порядковых числительных.
    \end{enumerate}

    Удаление символа «№»:
    \begin{minted}{text}
cleaned = re.sub(r'№', '', cleaned)
    \end{minted}

    Также были удалены скобки с числами и одиночных чисел на конце/начале
    \begin{minted}{text}
cleaned = re.sub(r'\([^)]*\d+[^)]*\)', '', cleaned)
cleaned = re.sub(r'\s+\d+\s*\.?$', '', cleaned)
cleaned = re.sub(r'^\d+\s+', '', cleaned)
    \end{minted}

    \begin{enumerate}
        \item \verb|\([^)]*\d+[^)]*\)| — удаляет любые круглые скобки, в которых есть цифры, например (том 2), (№10, переработанное);
        \item \verb|\s+\d+\s*\.?$| — удаляет числа, стоящие в конце строки, возможно с точкой;
        \item \verb|^\d+\s+| — удаляет ведущие числа в начале строки.
    \end{enumerate}

    После всего были удалены прочие нежелательные символы и нормализированы пробелы

    \begin{minted}{text}
cleaned = re.sub(r'[^\w\s.,!?-]', '', cleaned)
cleaned = re.sub(r'\s+', ' ', cleaned)
cleaned = cleaned.strip(' ,.-')
    \end{minted}

    \begin{enumerate}
        \item \verb~r'[^\w\s.,!?-]'~ — удаляет всё, кроме букв / цифр / подчёрки (\verb~\w~), пробелов, и набора разрешённых знаков . , ! ? -.
        Это убирает лишние спецсимволы, кавычки, другие скобки, символы валют и т.п.
        \item Затем сводим последовательные пробелы к одному и убираем ведущие / замыкающие пробелы и пунктуацию , . -.
    \end{enumerate}

    В результате очистки было изменено 8666 названий из 32085.

    После предобработки были просмотрены пропуски в данных.
    Просто NaN не было, зато встречались тексты и названия, в которых не было никаких символов.
    Также в текстах часто встречалась заглушка «описание отсутствует».
    Все такие строки были удалены.

    \begin{minted}{text}
data = data[data.text != ""].reset_index(drop=True)
data = data[data.title != ""].reset_index(drop=True)
data = data[data.text != "описание отсутствует"].reset_index(drop=True)
    \end{minted}

    В некоторых строках встречался нерусский текст.
    Так как датасет собирается для модели генерации на русском языке, такие строки были почищены, чтобы не вносить модели лишний шум.
    Для этого была написана функция \texttt{keep\_only\_russian}, которая оставляла только русские символы, цифры и пунктуацию в тексте.
    Если после этого появлялись пропуски, то они удалялись.

    \begin{minted}{text}
def keep_only_russian(text):
    if pd.isna(text):
        return ""
    text = re.sub(r"[^А-Яа-яЁё0-9\s.,!?-]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

data.text = data.text.apply(keep_only_russian)
data.cleaned_title = data.cleaned_title.apply(keep_only_russian)

data = data[(data.text != "") & (data.cleaned_title != "")].reset_index(drop=True)
    \end{minted}

    После первичной обработки данных были дополнительно проверены строки, содержащие текст.
    В датасете встречались случаи, когда строка формально не была пустой, однако не содержала никаких значимых символов —
    только пробелы или пунктуацию.
    Такие строки не несут полезной информации и потенциально могут добавить шум при обучении языковой модели.

    Чтобы отфильтровать подобные случаи, была написана вспомогательная функция is\_meaningful, которая проверяет,
    содержит ли текст хотя бы один букво-цифровой символ (на русском или английском языке).

    Также при парсинге на некоторых сайтах в начале текста встречалась лишняя ведущая пунктуация (например, «…», «–––», «***»).
    Для устранения этого была реализована функция \texttt{clean\_leading\_punct}, которая удаляет начальные небуквенные символы с помощью
    регулярного выражения.

    После применения этих функций датасет очищается от нерелевантных строк.
    Далее выполняется нормализация текста:
    \begin{enumerate}
        \item Замена всех символов, не относящихся к слову, пробелу или базовой пунктуации, на пробел;
        \item Устранение избыточных пробелов.
    \end{enumerate}

    В результате тексты становятся более однородными и удобными для дальнейшей обработки.

    \begin{minted}{text}
def is_meaningful(text):
    return bool(re.search(r"[А-Яа-яA-Za-z0-9]", text))

def clean_leading_punct(text):
    return re.sub(r"^[^\wА-Яа-я0-9]+", "", text).strip()

data = data[data.text.apply(is_meaningful)].reset_index(drop=True)
data.text = data.text.str.replace(r"[^\w\s,.!?-]", " ", regex=True)
data.text = data.text.str.replace(r"\s+", " ", regex=True).str.strip()
    \end{minted}

    После нормализации была проведена проверка на дубликаты.
    Сначала было посчитано их общее количество (430), затем — количество повторяющихся значений отдельно по тексту (762) и названию (2127).

    Оказалось, что дубликаты названий встречаются достаточно часто — это допустимо, так как одни и те же произведения
    могут быть размещены на разных сайтах и иметь одинаковые заголовки.

    Однако более критичны случаи, когда совпадает текст, а названия различаются.
    Это означает, что одно и то же произведение было спарсено несколько раз с разных источников.
    Такие строки следует удалить, чтобы не дублировать обучающие данные и не вносить перекос в модель.

    Для этого были удалены все повторяющиеся тексты, оставив от каждого дубликата только один экземпляр.

    \begin{minted}{text}
data.duplicated().sum()
data.text.duplicated().sum(), data.title.duplicated().sum()

data = data[~data.text.duplicated(keep=False)].reset_index(drop=True)
data.duplicated().sum()
    \end{minted}

    Таким образом, датасет был очищен от нерелевантных строк и повторяющихся текстов, что положительно скажется на качестве
    обучения генеративной модели.


    На следующем этапе был проведён анализ длины заголовков (названий произведений).
    Сначала было вычислено распределение частот появления каждого уникального заголовка, после чего для каждого названия
    была рассчитана длина — количество слов, содержащихся в нём.
    Это позволило сгруппировать данные и оценить, какие длины встречаются чаще всего.


    Полученное распределение визуализировано в виде столбчатой диаграммы.
    По графику \ref{fig:words_len} видно, что в датасете присутствуют названия, состоящие из большого количества слов.
    Такие случаи обычно характерны либо для метаданных, случайно попавших в заголовок, либо для чрезмерно подробных описаний.
    Поскольку задача — обучить модель генерировать короткие и понятные художественные названия,
    слишком длинные заголовки могут вносить шум и снижать качество обучения.

    \begin{minted}{text}
data["title_len"] = data.title.apply(lambda x: len(str(x).split()))
length_counts = data.groupby('title_len').size()

plt.figure(figsize=(12,6))
length_counts.plot(kind='bar')
plt.xlabel("Длина названия (слов)")
plt.ylabel("Количество названий")
plt.title("Распределение длин названий в датасете без аугментации")
plt.show()
    \end{minted}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{pic/words_len}
        \caption{Распределение длин названий в датасете без аугментации}\label{fig:words_len}
    \end{figure}


    После анализа было принято решение ограничить длину названий до 10 слов, поскольку более длинные примеры встречаются редко
    и не являются репрезентативными.
    Соответствующие строки были удалены, а датасет пересчитан.


    Далее были рассчитаны дополнительные статистики: количество примеров, число уникальных заголовков,
    средняя длина текста и средняя длина заголовков.
    Эти значения позволяют оценить «средний масштаб» данных и убедиться, что после фильтрации структура выборки остаётся
    адекватной для обучения модели.


    После этого вспомогательные столбцы title\_len и text\_len были удалены, а финальный датасет сохранён в CSV-файл для
    дальнейшего использования.


    Чтобы дополнительно проанализировать частотность слов в названиях произведений, было построено облако слов,
    объединяющее все заголовки в единый корпус.
    Полученная визуализация, представленная на рисунке \ref{fig:words_cloud} демонстрирует, какие слова встречаются чаще всего.

    \begin{minted}{text}
all_titles = " ".join(data.title.astype(str))
wordcloud = WordCloud(
    width=1200, height=600,
    background_color="white",
    max_words=400,
    colormap="viridis",
    collocations=False,
).generate(all_titles)
    \end{minted}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{pic/words_cloud}
        \caption{Облако слов по заголовкам}\label{fig:words_cloud}
    \end{figure}

    На облаке слов заметно, что значительную роль играют предлоги, союзы и другие служебные слова.
    Это логично, поскольку они естественным образом чаще всего встречаются в русских названиях.
    Хотя такие слова можно было бы исключить для получения более информативного облака, в данном случае было решено оставить их,
    чтобы модель могла учиться более реалистичной структуре заголовков, приближённой к художественным текстам.

    \subsection{Аугментация данных}

    Следующим этапом работы с датасетом стала аугментация данных, направленная на увеличение объёма тренировочной выборки без
    непосредственного добавления новых источников.
    Это особенно важно при обучении моделей обработки естественного языка, так как наличие более широкого набора текстов
    способствует лучшей генерализации и снижению переобучения.

    Перед аугментацией исходные данные были корректно разделены на тренировочную и валидационную выборки с учётом ограничения
    на повторяющиеся названия.
    Это позволило избежать потенциальной утечки информации: валидационная выборка после повторного разбиения не содержит текстов
    с одинаковыми заголовками, что улучшает объективность последующей оценки модели.

    \begin{minted}{text}
train_df, val_df = train_test_split(data, test_size=0.2, random_state=42)
    \end{minted}

    После предварительного анализа стало видно, что в первоначальную валидационную выборку попали несколько текстов
    с одинаковыми названиями.
    Это создаёт риск искажения оценки качества модели, если одно и то же произведение (или его вариации) встречается и в обучении,
    и в валидации, модель может фактически «узнать» его структуру, а не обобщать.

    Чтобы избежать подобной утечки данных, было выполнено контролируемое разделение датасета.

    Для этого реализована функция \texttt{split\_with\_controlled\_test\_size}, которая:
    \begin{enumerate}
        \item Группирует записи по названию произведения.
        \item Перемешивает список уникальных заголовков.
        \item Для каждого заголовка случайным образом выбирает ровно один пример в валидационную выборку (пока не будет набран заданный размер).
        \item Остальные примеры заголовка попадают в обучение.
    \end{enumerate}


    Таким образом мы гарантируем, что один и тот же заголовок не встречается в обеих выборках одновременно,
    что обеспечивает более честную оценку генеративной модели.

    \begin{minted}{text}
def split_with_controlled_test_size(data, target_test_size=0.2, random_state=42):
    np.random.seed(random_state)

    title_groups = data.groupby('title').apply(lambda x: x.index.tolist()).to_dict()

    unique_titles = list(title_groups.keys())
    np.random.shuffle(unique_titles)

    train_indices = []
    test_indices = []

    target_test_count = int(len(data) * target_test_size)

    for title in unique_titles:
        indices = title_groups[title]

        if len(test_indices) < target_test_count:
            test_idx = np.random.choice(indices, 1)[0]
            test_indices.append(test_idx)
            train_indices.extend([idx for idx in indices if idx != test_idx])
        else:
            train_indices.extend(indices)

    return data.iloc[train_indices], data.iloc[test_indices]
    \end{minted}


    После выполнения функции было выведено распределение размеров выборок, что позволило убедиться в корректности деления:
    \begin{minted}{text}
Общий размер данных: 29815
Тренировочная выборка: 23852 записей (80.0%)
Валидационная выборка: 5963 записей (20.0%)
    \end{minted}






    После формирования тренировочной выборки был выполнен этап аугментации.
    Важно подчеркнуть, что аугментировать нужно только train-набор, чтобы избежать утечки информации в валидацию,
    которая служит для объективной оценки качества модели.

    Поэтому перед началом процедуры индекс тренировочного датасета был сброшен.


    Для увеличения количества обучающих примеров текст каждой записи был разбит на небольшие смысловые фрагменты. Разбиение происходит по предложениям с фиксированной длиной блока: в данном случае — по три предложения на каждый фрагмент. Это позволяет:
    \begin{enumerate}
        \item увеличить объём обучающих данных;
        \item дать модели больше разнообразия контекста;
        \item улучшить способность генерации коротких связных отрывков.
    \end{enumerate}

    Для этого была реализована функция \texttt{split\_text\_into\_chunks}, использующая токенизацию предложений:

    \begin{minted}{text}
def split_text_into_chunks(text, sentences_per_chunk=3):
    sentences = sent_tokenize(text, language="russian")
    chunks = []
    for i in range(0, len(sentences), sentences_per_chunk):
        chunk = " ".join(sentences[i:i + sentences_per_chunk])
        chunks.append(chunk)
    return chunks
    \end{minted}


    Каждый chunk получает тот же заголовок, что и оригинальный текст, так как принадлежит одному произведению.

    В итоге формируется новый датасет augmented\_dataset, размер которого значительно превышает исходный train-набор.


    После генерации chunk’ов был выполнен знакомый по предыдущим этапам {этап очистки}

    \begin{enumerate}
        \item Удаление строк, не содержащих значимых символов (букв или цифр);
        \item Удаление лишней пунктуации в начале строки;
        \item Замена нежелательных символов на пробел;
        \item Нормализация пробелов и удаление хвостовых отступов.
    \end{enumerate}


    После разбиения некоторые chunk’и оказались слишком короткими или малоинформативными.
    Такие строки не несут полезной контекстной нагрузки и могут ухудшить качество обучения, особенно при генерации связного текста.

    Поэтому было добавлено дополнительное условие фильтрации:
    \begin{minted}{text}
augmented_dataset = augmented_dataset[augmented_dataset.text.str.strip().str.len() > 10]
    \end{minted}


    Это позволяет:
    \begin{enumerate}
        \item Отсеять «обрывки»;
        \item Исключить заглушки;
        \item Сохранить только содержательные обучающие примеры.
    \end{enumerate}


    После аугментации тренировочных данных (разбиения текстов на фрагменты) образовался существенно расширенный набор записей.
    Однако дальнейший анализ выявил ряд потенциальных проблем, которые необходимо устранить, чтобы избежать ухудшения качества
    обучения модели.

    Из-за разбиения длинных текстов на множество фрагментов отдельные заголовки (title) начали появляться слишком часто.
    Это создает дисбаланс и может привести модель к переобучению на популярные заголовки, снижая её способность обобщать.

    Чтобы равномерно распределить данные, было принято решение оставить не более 500 фрагментов на каждый заголовок:
    \begin{minted}{text}
max_per_title = 500
augmented_dataset = augmented_dataset.groupby("title").head(max_per_title).reset_index(drop=True)
    \end{minted}


    Таким образом, наиболее длинные произведения перестают доминировать в обучающем корпусе, а модель получает более
    стабильный распределённый по заголовкам набор данных.


    После обрезки по количеству записей появился небольшой процент точных дубликатов, где совпадали и text, и title.
    Такие записи никак не обогащают датасет и фактически дублируют сигнал для модели, поэтому были удалены.
    Это позволяет снизить помощь модели «запоминать» конкретные фрагменты.


    Следующий обнаруженный источник шума — случаи, когда один и тот же текстовый фрагмент встречается под разными названиями.
    Это может привести к обучению модели ложным связям: одинаковый контент начинает ассоциироваться с разными заголовками,
    что затрудняет обучение генерации.

    Такие записи были удалены с помощью фильтрации по дубликатам на уровне столбца text:
    \begin{minted}{text}
augmented_dataset = augmented_dataset[~augmented_dataset.text.duplicated(keep=False)].reset_index(drop=True)
    \end{minted}


    В завершение был выполнен дополнительный анализ:
    \begin{minted}{text}
dup_titles = (
    augmented_dataset.groupby("text")["title"]
    .nunique()
    .reset_index()
    .query("title > 1")
)
    \end{minted}


    Он показывает, сколько фрагментов всё ещё имеют более одного уникального заголовка.
    После проведённой фильтрации количество таких случаев больше не было, что подтверждает корректность предпринятых действий.


    После выполнения аугментации тренировочного набора данных было важно оценить, как изменилось распределение длин заголовков (в словах).
    Для этого были рассчитаны длины названий в трёх выборках:
    \begin{itemize}
        \item Исходный датасет без аугментации;
        \item Тренировочная выборка после аугментации;
        \item Валидационная выборка.
    \end{itemize}

    Измерение длины выполнялось по количеству слов в заголовке.


    Далее была построена сравнительная гистограмма, представленная на рисунке \ref{fig:augmentation}, отображающая распределение полученных значений.

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{pic/augmentation}
        \caption{Сравнение распределения длин названий до и после аугментации}\label{fig:augmentation}
    \end{figure}


    По графику можно заметить следующие особенности:

    \begin{enumerate}
        \item Исходное распределение
        \begin{itemize}
            \item Распределение достаточно равномерное в области коротких названий.
            \item Количество примеров сравнительно небольшое, так как это необработанный датасет без разбиения текста на части.
        \end{itemize}

        \item train после аугментации
        \begin{itemize}
            \item Сильно увеличилось общее количество примеров.
            \item Распределение визуально сохраняет схожую форму: короткие названия встречаются значительно чаще, чем длинные.
            \item Это закономерно, так как аугментация увеличивает количество текстовых фрагментов, но {название} у каждого chunk’а остаётся прежним.
        \end{itemize}

        \item {test/val после аугментации}
        \begin{itemize}
            \item Количество названий здесь существенно меньше, так как валидация purposely не аугментируется.
            \item Распределение по длинам заголовков остаётся очень близким к исходному, что важно для корректной оценки модели.
        \end{itemize}
    \end{enumerate}


    \section{Написание и обучение моделей}

    \subsection{Модель на основе n-gram}


    Для подготовки текста к обучению модели используется функция {tokenize}, реализующая базовый этап предобработки входных данных.
    Её задача — преобразовать сырой текст в последовательность токенов,
    пригодных для последующего анализа и построения n-граммной модели.

    На первом шаге весь текст приводится к нижнему регистру.
    Это устраняет различия между одинаковыми словами, записанными с разным использованием заглавных букв, и уменьшает размер словаря.

    Затем выполняется очистка строки с помощью регулярного выражения.
    Из текста удаляются все символы, не являющиеся буквами латиницы или кириллицы, цифрами или пробелами.

    После очистки строка разбивается на отдельные токены по пробелам.
    Результатом работы функции является список слов, упорядоченных в соответствии с исходным текстом.
    Эти токены далее используются для построения обучающих последовательностей и формирования n-грамм.

    \begin{minted}{text}
def tokenize(text: str) -> List[str]:
    """Функция самой простой предобработки текста, основанной на разбиении на токены по пробелам."""
    text = text.lower()
    text = re.sub(r"[^a-zа-яё0-9\s]", "", text)
    return text.split()
    \end{minted}


    Для решения задачи генерации названий по входному тексту используется класс {TitleNgramModel},
    реализующий статистическую {n-граммную модель}.
    Она обучается на парах {(текст + название)} и оценивает вероятность появления следующего слова на основе нескольких предыдущих.

    В методе \textff{\_\_init\_\_} задаются основные настройки и внутренние структуры данных, необходимые для работы модели:


    \begin{enumerate}
        \item {n\_gram} — порядок модели, определяющий, сколько предыдущих токенов используется как контекст при
        прогнозировании следующего слова.
        Например, при n\_gram = 3 модель опирается на биграммный контекст;
        \item {ngrams} — словарь, в котором каждому контексту сопоставляется счётчик слов, появляющихся после него.
        Это ядро модели: оно хранит частоты всех наблюдённых n-грамм;
        \item {context\_counts} — количество встреч каждого контекста;
        Используется при вычислении вероятностей;
        \item {vocab} — словарь уникальных токенов, включающий наиболее частотные слова корпуса и специальные служебные символы;
        \item {Специальные токены}:
        \begin{itemize}
            \item <s> — начало последовательности;
            \item <title> — разделитель, отделяющий исходный текст от целевого названия;
            \item </s> — конец последовательности;
            \item <unk> — токен для слов, отсутствующих в словаре.
        \end{itemize}
        \item {smoothing} — стратегия сглаживания вероятностей (по умолчанию используется Лапласовское сглаживание).
        Сглаживание предотвращает нулевые вероятности для слов, отсутствующих в обучающих n-граммах;
        \item {alpha} — коэффициент, применяемый в формулах сглаживания;
        \item {lower\_order\_models} — модели меньшего порядка
        (например, для 4-граммной модели автоматически создаются 1-, 2- и 3-граммные версии).
        Они используются для методов типа {linear interpolation} или {backoff},
        позволяющих улучшать генерацию при редких или отсутствующих контекстах.
    \end{enumerate}

    \begin{minted}{text}
class TitleNgramModel:
    """Модель n-грамм, обучающаяся по парам (текст → название)."""

    def __init__(self, n_gram: int = 4, smoothing: str | None = 'laplace', alpha: float = 1.):
        self.n_gram = n_gram
        self.ngrams = defaultdict(Counter)
        self.context_counts = defaultdict(int)
        self.vocab = set()
        self.start_token = "<s>"
        self.title_token = "<title>"
        self.end_token = "</s>"
        self.unknown_token = "<unk>"
        self.smoothing = smoothing
        self.alpha = alpha
        self.lower_order_models = {}
    \end{minted}


    Метод create\_ngrams отвечает за построение последовательностей n-грамм, которые позднее используются при обучении модели.
    Он принимает на вход список токенов и преобразует его в набор перекрывающихся отрезков длины {n},
    каждый из которых описывает локальный контекст в тексте.

    \begin{minted}{text}
def create_ngrams(self, tokens: List[str]) -> list[tuple[str, ...]]:
    """Создание n-грамм с учётом начала и конца."""
    tokens = [self.start_token] + tokens + [self.end_token]
    return [tuple(tokens[i:i + self.n_gram]) for i in range(len(tokens) - self.n_gram + 1)]
    \end{minted}

    Дальше идет функция полного обучения моделей.

    Метод train выполняет полное построение n-граммной модели на основе набора пар (текст \to название).
    Процесс обучения состоит из нескольких последовательных этапов.

    \begin{enumerate}
        \item Токенизация данных и формирование общего корпуса: \\
        Первым шагом выполняется преобразование исходных пар в единый поток токенов.
        Для каждого текста и соответствующего ему названия выполняется разбиение на слова, после чего элементы объединяются
        в одну последовательность, разделённую специальным маркером <title>.
        \begin{minted}{text}
all_tokens = []
for text, title in pairs:
    text_tokens = tokenize(text)
    title_tokens = tokenize(title)
    combined = text_tokens + [self.title_token] + title_tokens
    all_tokens.extend(combined)
        \end{minted}

        \item Формирование словаря токенов (vocabulary): \\
        После сбора всех токенов подсчитываются частоты слов, и формируется словарь модели.
        В него включаются только те слова, которые встретились более одного раза, что помогает уменьшить число редких элементов.
        \begin{minted}{text}
token_counts = Counter(all_tokens)
self.vocab = set(token for token, count in token_counts.items() if count > 1)
self.vocab.update([self.start_token, self.title_token, self.end_token, self.unknown_token])
        \end{minted} \\
        Слова, отсутствующие в словаре, будут заменяться на специальный токен <unk>.

        \item Построение моделей меньшего порядка:

        Если основная модель использует n-граммы порядка n > 1, то автоматически создаются модели всех меньших порядков (1-граммная, 2-граммная и т.д.).

        Они будут применяться для линейной интерполяции или backoff-алгоритмов.

        \begin{minted}{text}
if self.n_gram > 1:
for n in range(1, self.n_gram):
    self.lower_order_models[n] = TitleNgramModel(n, self.smoothing, self.alpha)
    self.lower_order_models[n].train(pairs)
        \end{minted}

        \item Подготовка данных и построение n-грамм: \\
        На следующем этапе текст и заголовок повторно токенизируются, но теперь с учетом словаря: редкие слова заменяются <unk>.
        \begin{minted}{text}
text_tokens = [token if token in self.vocab else self.unknown_token for token in text_tokens]
title_tokens = [token if token in self.vocab else self.unknown_token for token in title_tokens]
combined = text_tokens + [self.title_token] + title_tokens
ngrams_list = self.create_ngrams(combined)
        \end{minted}

        \item Подсчёт частот контекстов и слов: \\
        Для каждой n-граммы фиксируется её контекст — первые n−1 слов — и следующее слово.
        Модель накапливает статистику, необходимую для расчёта вероятностей.
        \begin{minted}{text}
for ngram in ngrams_list:
    context = ngram[:-1]
    next_word = ngram[-1]
    self.ngrams[context][next_word] += 1
    self.context_counts[context] += 1
        \end{minted}
        В результате модель получает полный набор частот, на которых далее строятся вероятности предсказания следующего слова.
    \end{enumerate}

    Для удобства работы с датасетом реализован вспомогательный метод train\_from\_csv, который выполняет загрузку данных,
    их фильтрацию и передачу в основной тренировочный цикл модели.
    Метод обеспечивает удобную интеграцию n-граммной модели с табличными корпусами.

    На вход подаётся путь к CSV-файлу, содержащему пары {(текст, название)}.
    Файл читается в формате pandas.DataFrame:
    \begin{minted}{text}
df = pd.read_csv(csv_path)
if limit:
    df = df.head(limit)
    \end{minted}

    Параметр limit позволяет ограничить размер обучающей выборки, что полезно на этапе отладки.

    Далее осуществляется проход по строкам таблицы.

    Используется индикатор выполнения tqdm, который позволяет отслеживать сколько осталось времени до полного прохода.

    \begin{minted}{text}
pairs = []
for _, row in tqdm(df.iterrows(), total=len(df), desc="Обучение модели"):
    text = getattr(row, text_col, None)
    title = getattr(row, title_col, None)
    if text and title:
        pairs.append((text, title))
    \end{minted}

    В каждой строке извлекаются значения из столбцов text и title (их имена можно менять через параметры функции).

    Если в строке отсутствует одно из полей, пара пропускается.

    В результате формируется список, полностью соответствующий тому формату данных, который использует основной метод обучения train.

    После подготовки пар выполняется вызов базового алгоритма обучения.

    В n-граммной модели ключевым этапом является оценка условной вероятности появления следующего слова при заданном контексте.
    В реализации предусмотрено несколько вариантов вычисления вероятностей: без сглаживания, со сглаживанием Лапласа и с
    линейной интерполяцией.
    Каждый из них решает разные проблемы статистической модели.

    \begin{enumerate}
        \item Простая вероятностная оценка (без сглаживания):
        \begin{minted}{text}
    def get_simple_probability(self, context: tuple, word: str) -> float:
        context_counts = self.ngrams.get(context, {})
        total = sum(context_counts.values())
        return context_counts[word] / total if total else 0.0
        \end{minted}

        Вероятность оценивается по формуле:
        $P(w_t \mid context) = \frac{C(context, w_t)}{C(context)}$
        где
        \begin{itemize}
            \item $C(context, w_t)$ — число раз, когда после данного контекста появилось слово $w_t$,
            \item $C(context)$ — сколько раз этот контекст встречался в данных.
        \end{itemize}
        Особенности:
        \begin{itemize}
            \item Это самый точный вариант для часто встречающихся n-грамм.
            \item Главный недостаток — нулевые вероятности для комбинаций, которые не встречались в обучении.
            При генерации текста это приводит к «обрыву» предсказаний.
        \end{itemize}
        \item Сглаживание Лапласа (Add-one): \\
        Для борьбы с нулевыми вероятностями используется метод Лапласовского сглаживания:
        \begin{minted}{text}
def get_laplace_probability(self, context: tuple, word: str) -> float:
    context_counts = self.ngrams.get(context, {})
    total = sum(context_counts.values())
    vocab_size = len(self.vocab)

    count_word = context_counts.get(word, 0)
    return (count_word + self.alpha) / (total + self.alpha * vocab_size)
        \end{minted}

        Принцип метода: \\
        Сглаживание Лапласа добавляет единицу к частоте каждого слова:
        $P(w_t \mid context) = \frac{C(context, w_t) + \alpha}{C(context) + \alpha \cdot |V|}$
        где
        \begin{itemize}
            \item \alpha = 1 (по умолчанию),
            \item |V| — размер словаря.
        \end{itemize}
        \item Линейная интерполяция (с моделями меньшего порядка): \\
        Метод \texttt{get\_linear\_interpolation\_probability} улучшает качество модели за счёт комбинирования вероятностей разных порядков:
        \begin{minted}{text}
def get_linear_interpolation_probability(self, context: tuple, word: str) -> float:
    if self.n_gram == 1 or not self.lower_order_models:
        return self.get_laplace_probability(context, word)

    lambda_current = 0.6
    lambda_backoff = 0.4

    current_prob = self.get_laplace_probability(context, word)

    backoff_context = context[1:] if len(context) > 1 else tuple()
    backoff_model = self.lower_order_models[self.n_gram - 1]
    backoff_prob = backoff_model.get_probability(backoff_context, word)

    return lambda_current * current_prob + lambda_backoff * backoff_prob
        \end{minted}

        Математическая формула: $P = \lambda \, P_n + (1 - \lambda) \, P_{n-1}$, где
        \begin{itemize}
            \item $P_n$ — вероятность в модели порядка n,
            \item $P_{n-1}$ — вероятность в модели меньшего порядка (n−1),
            \item $\lambda$ — вес старшей модели.
        \end{itemize}
        Так как редкие контексты приводят к недостоверным оценкам.
        Интерполяция позволяет «страховаться» моделями меньшего порядка, которые более устойчивы статистически.
        \item Универсальный метод выбора вероятности:

        Общий интерфейс \texttt{get\_probability} выбирает метод в зависимости от параметров модели:
        \begin{minted}{text}
def get_probability(self, context: tuple, word: str) -> float:
    if word not in self.vocab:
        word = self.unknown_token

    if self.smoothing == 'laplace':
        return self.get_laplace_probability(context, word)
    elif self.smoothing == 'linear':
        return self.get_linear_interpolation_probability(context, word)
    elif self.smoothing is None:
        return self.get_simple_probability(context, word)
    else:
        return self.get_laplace_probability(context, word)
        \end{minted}
    \end{enumerate}

    Далее идут методы для генерации названия.

    Метод \texttt{generate\_with\_backoff} вычисляет распределение вероятностей слов для заданного контекста,
    используя стратегию постепенного уменьшения длины контекста.

    Если полная n-грамма отсутствует в тренировочных данных, метод постепенно сокращает контекст, пока не найдёт подходящий.

    Если подходящего контекста нет вовсе, используется равномерное распределение по словарю.

    \begin{enumerate}
        \item Инициализация контекста и предельной глубины отката: \\
        Сначала фиксируется исходный контекст и максимальная допустимая глубина backoff-уменьшения.

        Если глубина не указана вручную, она устанавливается равной n - 1, что позволяет модели последовательно пройти
        через все возможные контексты меньшего порядка.

        \begin{minted}{text}
if max_depth is None:
    max_depth = self.n_gram - 1
current_context = context
depth = 0
        \end{minted}

        \item Проверка наличия контекста в модели:

        На каждом шаге выполняется проверка: встречался ли текущий контекст в обучающих данных.
        Если да — можно вычислять вероятности слов, следующих за этим контекстом.
        \begin{minted}{text}
if current_context in self.ngrams:
    probabilities = dict()
        \end{minted}

        \item Вычисление вероятностей слов для текущего контекста:

        Для каждого слова из словаря, за исключением служебных токенов, вычисляется вероятность по выбранной стратегии сглаживания:

        \begin{minted}{text}
for word in self.vocab:
    if word not in [self.title_token, self.title_token, self.end_token]:
        probabilities[word] = self.get_probability(current_context, word)
        \end{minted}

        \item Нормировка распределения:

        После расчёта суммируются все вероятности.
        Если сумма положительна, выполняется нормировка, чтобы получить корректное вероятностное распределение, сумма которого равна 1.
        \begin{minted}{text}
total_prob = sum(probabilities.values())
if total_prob > 0:
    return {word: prob / total_prob for word, prob in probabilities.items()}
        \end{minted}
        \item При отсутствии данных — уменьшение контекста (backoff):

        Если ни один след за данным контекстом не зафиксирован, модель постепенно сокращает контекст, удаляя первый токен.

        Процесс продолжается, пока:
        \begin{enumerate}
            \item не будет найдено подходящее состояние;
            \item или не будет достигнута максимальная глубина отката;
            \item или контекст не станет однословным.
        \end{enumerate}

        \begin{minted}{text}
if len(current_context) > 1:
    current_context = current_context[1:]
else:
    break
depth += 1
        \end{minted}
        \item Фоллбек: равномерное распределение по словарю:

        Если ни один контекст (ни меньшего, ни полного порядка) не встречался в корпусе, применяется равномерное распределение
        по всем доступным словам словаря, за исключением служебных токенов.

        \begin{minted}{text}
words = [w for w in self.vocab if w not in [self.start_token, self.title_token, self.end_token]]
return {word: 1.0 / len(words) for word in words}
        \end{minted}
    \end{enumerate}

    Метод \texttt{generate\_title} выполняет автоматическое порождение заголовка на основе входного текста.

    Он использует вероятностную модель n-грамм, backoff-алгоритм и выбранный метод сглаживания.

    Процесс генерации включает несколько последовательных этапов.

    \begin{enumerate}
        \item Токенизация входного текста и нормализация токенов:

        На первом шаге входная строка разбивается на токены.
        Каждый токен проверяется на принадлежность словарю модели:
        если токен отсутствует в словаре, он заменяется специальным маркером <unk>.

        После текста добавляется маркер <title>, обозначающий начало заголовка.

        \begin{minted}{text}
tokens = tokenize(input_text)
tokens = [token if token in self.vocab else self.unknown_token for token in tokens]
tokens += [self.title_token]
        \end{minted}


        \item Формирование исходного контекста:

        Для старта генерации используется последние n-1 токенов исходной последовательности, куда входит маркер <title>.

        Это обеспечивает корректную условность предсказываемого слова: $P(w_t | \text{context})$.
        \item Итеративное предсказание следующего слова:

        Генерация выполняется максимум в течение max\_words шагов.

        На каждом шаге рассчитывается вероятностное распределение следующих слов с помощью backoff-механизма.

        \begin{minted}{text}
word_probs = self.generate_with_backoff(context)
if not word_probs:
    break
        \end{minted}

        \item Нормировка распределения:

        Backoff возвращает ненормированное распределение.

        Сначала вычисляется сумма вероятностей, затем каждое значение нормируется так, чтобы сумма равнялась 1.

        \begin{minted}{text}
word_probs = {word: prob for word, prob in word_probs.items()}
total = sum(word_probs.values())
word_probs = {word: prob / total for word, prob in word_probs.items()}
        \end{minted}
        \item Стохастический выбор следующего слова:

        Следующее слово выбирается случайно, пропорционально вероятностям.

        Используется стандартная выборка с весами.

        \begin{minted}{text}
words = list(word_probs.keys())
probs = list(word_probs.values())
next_word = random.choices(words, weights=probs)[0]
        \end{minted}

        Если модель сгенерировала маркеры </s> или <title>, процесс завершатся — это признаки конца последовательности.

        \begin{minted}{text}
if next_word in {self.end_token, self.title_token}:
    break
        \end{minted}

        \item Обновление контекста для следующего шага:

        Добавленное слово включается в контекст, который смещается на один шаг вправо.

        Таким образом, в каждый момент времени используется актуальная n-1-грамма.

        \begin{minted}{text}
result.append(next_word)
context = (*context[1:], next_word) if len(context) > 0 else (next_word,)
        \end{minted}
        \item Формирование итогового заголовка:

        Полученные слова объединяются в строку.
        Если модель не смогла сгенерировать ни одного слова, возвращается fallback-значение «Без названия».

        \begin{minted}{text}
title = “ “.join(result)
return title if title else “Без названия”
        \end{minted}
    \end{enumerate}


    Для оценки качества работы модели, основанной на n-граммах, использовалась только одна метрика — METEOR,
    поскольку она наиболее подходит для оценки качества коротких текстов и учитывает как точные совпадения слов,
    так и семантическую близость.

    Метод \texttt{evaluate\_meteor} вычисляет среднее значение METEOR-метрики для заданного тестового набора.

    Эта метрика позволяет количественно оценить качество генерации заголовков, сопоставляя с эталонными (reference) заголовками.

    \begin{minted}{text}
def evaluate_meteor(self, test_pairs: List[Tuple[str, str]]) -> float:
    """Вычисление средней METEOR-оценки для тестового набора (text, reference_title)."""
    scores = []
    for text, true_title in tqdm(test_pairs, desc="Оценка METEOR"):
        generated = self.generate_title(text)
        score = meteor_score([tokenize(true_title)], tokenize(generated))
        scores.append(score)
    return sum(scores) / len(scores) if scores else 0.0
    \end{minted}


    Реализованный класс TitleNgramModel содержит методы save и load, которые позволяют сериализовать
    модель на диск и восстановить её из файла.

    Для этого используется стандартный модуль Python — pickle.
    \begin{minted}{text}
def save(self, path: str):
    with open(path, "wb") as f:
    pickle.dump(self, f)

@staticmethod
def load(path: str):
    with open(path, "rb") as f:
        return pickle.load(f)
    \end{minted}

    Для обучения n-граммной модели использовался заранее подготовленный набор данных, содержащий пары (текст + заголовок).
    Обучающая выборка была загружена из CSV-файла, после чего модель была обучена и сохранена для последующего использования.

    Модель была создана с использованием триграмм (n = 3).
    Такой размер контекста позволяет учитывать два предыдущих слова при предсказании следующего.

    После запуска процесса обучения был сформирован полный набор n-грамм и произведена оценка качества генерации на
    валидационном наборе.

    В ходе обучения были получены следующие результаты:

    \begin{enumerate}
        \item Было обработано 821612 обучающих примеров.
        \item По итогам обучения модель сформировала 11126709 уникальных контекстов.
    \end{enumerate}


    После завершения обучения была проведена оценка качества генерации заголовков с использованием метрики METEOR на
    валидационном наборе объёмом 5963 примера.
    В результате средняя METEOR оценка составила 0.

    Это показывает, что сгенерированные моделью заголовки практически не совпадают с референсными.
    Это ожидаемо для простой статистической модели.


    Для демонстрации работы модели были выполнены запросы на генерацию названий для нескольких произвольных текстов.
    Как видно из примеров, модель формирует последовательности слов, которые не связаны по смыслу с содержанием текста.
    Это подтверждает ограниченность n-граммного подхода в задачах семантической генерации.


    \textbf{Пример 1}

    Исходный текст (фрагмент):
    «У меня большая семья из шести человек: я, мама, папа, старшая сестра, бабушка и дедушка…»

    Сгенерированное название:
    «свиста ниппур апокалиптические каллиграфическими кастрированный благословление гвардейскую»


    \textbf{Пример 2}

    Исходный текст (фрагмент):
    «Я с детства хотел завести собаку, но родители мне не разрешали…»

    Сгенерированное название:
    «местечковая постройнела просыпали унижениями расчерченные направляюттолкают многотысячными»


    \textbf{Пример 3}

    Исходный текст (фрагмент):
    «Когда я окончил университет, то начал ходить по собеседованиям в разные компании…»

    Сгенерированное название:
    «запястий эмбриологии залишилась крючьев сочиняющие айви зацветающих»

    Сгенерированные заголовки почти полностью состоят из редких или случайных слов, часто морфологически несовместимых.
    У них отсутствуют: смысловые связи, тематическая релевантность, грамматическая структура.

    Поведение модели объясняется тем, что n-граммная модель:
    \begin{enumerate}
        \item Работает только на поверхностных статистических закономерностях;
        \item Не понимает смысла текста;
        \item Формирует заголовки по принципу вероятностного продолжения последовательностей, встретившихся в корпусе.
    \end{enumerate}

    \conclusion

    \newpage


% Отобразить все источники. Даже те, на которые нет ссылок.
    \nocite{*}

    \bibliographystyle{ugost2003}
    \bibliography{thesis}

    \appendix


    \section{Парсинг}
    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/parsing/parsing_proza_ru.py}
    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/parsing/parsing_briefly.py}
    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/parsing/parsing_litprichal.py}
    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/parsing/parsing_litres.py}


    \section{Работа с данными}
    \inputminted{py}{code/work_with_all_data.py}
    \inputminted{py}{code/augmentation.py}


    \section{Модель на основе n-gram}
    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/my_models/history/n-gram/n_gram_model.py}
    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/my_models/history/n-gram/n-gram_train.py}
    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/my_models/history/n-gram/n-gram_test.py}

%    \section{Seq2seq}
%    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/my_models/Seq2seq/data/dataset.py}
%    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/my_models/Seq2seq/data/preprocess.py}
%    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/my_models/Seq2seq/data/vocab.py}
%    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/my_models/Seq2seq/model/decoder.py}
%    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/my_models/Seq2seq/model/encoder.py}
%    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/my_models/Seq2seq/model/seq2seq.py}
%    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/my_models/Seq2seq/config.py}
%    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/my_models/Seq2seq/inference.py}
%    \inputminted{py}{/Users/veronika_steklo/PycharmProjects/generation_names/my_models/Seq2seq/train.py}

\end{document}

