{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-19T19:00:14.189636Z",
     "start_time": "2025-07-19T19:00:14.130657Z"
    }
   },
   "source": [
    "import time\n",
    "import json\n",
    "import re\n",
    "from typing import Optional, Dict\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Парсинг Проза.ру",
   "id": "2f62deb36be27c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T19:00:18.222609Z",
     "start_time": "2025-07-19T19:00:18.204752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ProzaRuParser:\n",
    "    \"\"\"Парсер для сайта proza.ru\"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str = \"https://proza.ru/texts/list.html\", delay: float = 1.5):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\"User-Agent\": UserAgent().random}\n",
    "        self.timeout = 10\n",
    "        self.delay = delay\n",
    "        self.output_file = \"data/data_proza_ru.json\"\n",
    "\n",
    "        with open(self.output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def _get_page(self, url: str) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"Загружает страницу и возвращает BeautifulSoup-объект.\"\"\"\n",
    "        try:\n",
    "            print(f\"Загружается: {url}\")\n",
    "            time.sleep(self.delay)\n",
    "            response = requests.get(url, headers=self.headers, timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, \"html.parser\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Ошибка при загрузке {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_all_forms(self) -> Dict[str, str]:\n",
    "        \"\"\"Получает названия и ссылки на разделы с малыми формами.\"\"\"\n",
    "        soup = self._get_page(self.base_url)\n",
    "        if not soup:\n",
    "            return {}\n",
    "\n",
    "        works_block = soup.find('ul', attrs={'type': 'square', 'style': 'color:#404040'})\n",
    "        all_forms = works_block.find_all('ul', attrs={'type': 'square'})\n",
    "        data_all_forms = {}\n",
    "        for form in all_forms:\n",
    "            category = form.find_all('a')\n",
    "            for link in category:\n",
    "                title = link.text.strip()\n",
    "                full_link = \"https://www.proza.ru\" + link['href']\n",
    "                data_all_forms[title] = full_link\n",
    "\n",
    "        return data_all_forms\n",
    "\n",
    "    def get_text(self, url: str) -> str:\n",
    "        soup = self._get_page(url)\n",
    "        if not soup:\n",
    "            return \"\"\n",
    "\n",
    "        text = soup.find('div', attrs={'class': 'text'})\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        return self.clean_text(str(text))\n",
    "\n",
    "    def clean_text(self, html: str) -> str:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for element in soup(['iframe', 'img', 'script', 'style',\n",
    "                             'div.video-blk', 'div.video-block',\n",
    "                             'div.ads', 'div.advertisement']):\n",
    "            element.decompose()\n",
    "\n",
    "        for div in soup.find_all('div', class_=lambda x: x and 'hidden' in x):\n",
    "            div.decompose()\n",
    "\n",
    "        clean_text = soup.get_text(separator='\\n', strip=True)\n",
    "        lines = [line.strip() for line in clean_text.split('\\n') if line.strip()]\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def get_works(self, url: str, category_title: str) -> Dict[str, Dict[str, str]]:\n",
    "        \"\"\"\n",
    "\n",
    "        :param url:\n",
    "        :param category_title:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        soup = self._get_page(url)\n",
    "        if not soup:\n",
    "            return {}\n",
    "\n",
    "        works_block = soup.find_all('ul', attrs={'type': 'square', 'style': 'color:#404040'})\n",
    "        data_small_works = {}\n",
    "\n",
    "        for works_list in works_block:\n",
    "            for work in works_list.find_all('li'):\n",
    "                work_data = work.find('a')\n",
    "                if not work_data:\n",
    "                    continue\n",
    "                try:\n",
    "                    author = work.find('a', attrs={'class': 'poemlink'}).text.strip()\n",
    "                    title = work_data.text.strip()\n",
    "                    link = \"https://www.proza.ru\" + work_data['href']\n",
    "                    text = self.get_text(link)\n",
    "\n",
    "                    data_small_works[author] = {\n",
    "                        'link': link,\n",
    "                        'title': title,\n",
    "                        'text': text\n",
    "                    }\n",
    "                    self._update_output_file(category_title, title, data_small_works[title])\n",
    "                    time.sleep(self.delay)\n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при обработке произведения: {e}\")\n",
    "                    continue\n",
    "\n",
    "        return data_small_works\n",
    "\n",
    "    def parse_by_dates(self, start_date: str, end_date: str, category_title: str, topic: str) -> Dict[str, dict]:\n",
    "        \"\"\"\n",
    "        Парсит материалы за указанный период в обратном порядке\n",
    "        :param category_title:\n",
    "        :param start_date: Дата начала в формате 'YYYY-MM-DD'\n",
    "        :param end_date: Дата окончания в формате 'YYYY-MM-DD'\n",
    "        :param topic: ID темы\n",
    "        :return: Словарь с данными\n",
    "        \"\"\"\n",
    "        current_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "        result = {}\n",
    "\n",
    "        while current_date >= end_date:\n",
    "            date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "            print(f\"\\nОбработка даты: {date_str}\")\n",
    "\n",
    "            day = current_date.strftime(\"%d\")\n",
    "            month = current_date.strftime(\"%m\")\n",
    "            year = current_date.strftime(\"%Y\")\n",
    "\n",
    "            url = f\"{self.base_url}?day={day}&month={month}&year={year}&topic={topic}\"\n",
    "            data_day = self.get_works(url, category_title)\n",
    "            result.update(data_day)\n",
    "\n",
    "            current_date -= timedelta(days=1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _update_output_file(self, category: str, title: str, work_data: dict):\n",
    "        \"\"\"Обновляет JSON-файл, добавляя новое произведение\"\"\"\n",
    "        try:\n",
    "            with open(self.output_file, 'r', encoding='utf-8') as f:\n",
    "                existing_data = json.load(f)\n",
    "\n",
    "            if category not in existing_data:\n",
    "                existing_data[category] = {}\n",
    "            existing_data[category][title] = work_data\n",
    "\n",
    "            with open(self.output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(existing_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка при обновлении файла: {e}\")\n",
    "\n",
    "    def get_all_work(self) -> Dict[str, Dict[str, Dict[str, str]]]:\n",
    "        \"\"\"Получает все малые формы и произведения внутри них.\"\"\"\n",
    "        all_data = {}\n",
    "        all_forms = self.get_all_forms()\n",
    "\n",
    "        for all_form_title, all_form_link in all_forms.items():\n",
    "            print(f\"\\nОбработка категории: {all_form_title}\")\n",
    "            works = self.get_works(all_form_link, all_form_title)\n",
    "            topic = re.search(r'topic=(\\d+)', all_form_link).group(1)\n",
    "            works_by_data = self.parse_by_dates(\"2025-07-18\", \"2025-07-11\", all_form_title, topic)\n",
    "            merged_works = {**works, **works_by_data}\n",
    "            all_data[all_form_title] = merged_works\n",
    "            time.sleep(self.delay)\n",
    "\n",
    "        return all_data\n",
    "\n",
    "    def save_to_json(self, data: dict, filename: str = \"data/data_proza_ru.json\"):\n",
    "        \"\"\"Сохраняет данные в JSON-файл.\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"\\n✅ Данные сохранены в {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ошибка при сохранении JSON: {e}\")\n",
    "\n"
   ],
   "id": "6ad908eeffdbeadf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-19T19:00:20.352316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parser = ProzaRuParser()\n",
    "parser.get_all_work()"
   ],
   "id": "452056176a3a4c33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружается: https://proza.ru/texts/list.html\n",
      "\n",
      "Обработка категории: миниатюры\n",
      "Загружается: https://www.proza.ru/texts/list.html?topic=05\n",
      "Загружается: https://www.proza.ru/2025/07/19/1660\n",
      "Загружается: https://www.proza.ru/2025/07/19/1653\n",
      "Загружается: https://www.proza.ru/2025/07/19/1644\n",
      "Загружается: https://www.proza.ru/2025/07/19/1641\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
